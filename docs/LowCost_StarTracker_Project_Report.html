<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>LowCost_StarTracker_Project_Report</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">LowCost_StarTracker_Project_Report</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a
href="#low-cost-star-tracker-a-software-based-approach-using-consumer-grade-hardware"
id="toc-low-cost-star-tracker-a-software-based-approach-using-consumer-grade-hardware">Low-Cost
Star Tracker: A Software-Based Approach Using Consumer-Grade
Hardware</a>
<ul>
<li><a href="#project-report" id="toc-project-report">Project
Report</a></li>
<li><a href="#abstract" id="toc-abstract">Abstract</a></li>
<li><a href="#table-of-contents" id="toc-table-of-contents">Table of
Contents</a></li>
<li><a href="#introduction" id="toc-introduction">1. Introduction</a>
<ul>
<li><a href="#background" id="toc-background">1.1 Background</a></li>
<li><a href="#problem-statement" id="toc-problem-statement">1.2 Problem
Statement</a></li>
<li><a href="#proposed-solution" id="toc-proposed-solution">1.3 Proposed
Solution</a></li>
<li><a href="#contributions" id="toc-contributions">1.4
Contributions</a></li>
</ul></li>
<li><a href="#literature-review" id="toc-literature-review">2.
Literature Review</a>
<ul>
<li><a href="#traditional-star-tracker-technology"
id="toc-traditional-star-tracker-technology">2.1 Traditional Star
Tracker Technology</a></li>
<li><a href="#gyroscope-based-stabilization"
id="toc-gyroscope-based-stabilization">2.2 Gyroscope-Based
Stabilization</a></li>
<li><a href="#image-stacking-techniques"
id="toc-image-stacking-techniques">2.3 Image Stacking
Techniques</a></li>
<li><a href="#low-cost-star-tracker-initiatives"
id="toc-low-cost-star-tracker-initiatives">2.4 Low-Cost Star Tracker
Initiatives</a></li>
<li><a href="#consumer-camera-astrophotography"
id="toc-consumer-camera-astrophotography">2.5 Consumer Camera
Astrophotography</a></li>
</ul></li>
<li><a href="#commercial-star-tracker-analysis"
id="toc-commercial-star-tracker-analysis">3. Commercial Star Tracker
Analysis</a>
<ul>
<li><a href="#market-overview" id="toc-market-overview">3.1 Market
Overview</a></li>
<li><a href="#commercial-products-analysis"
id="toc-commercial-products-analysis">3.2 Commercial Products
Analysis</a></li>
<li><a href="#cost-breakdown-analysis"
id="toc-cost-breakdown-analysis">3.3 Cost Breakdown Analysis</a></li>
<li><a href="#barrier-to-entry" id="toc-barrier-to-entry">3.4 Barrier to
Entry</a></li>
</ul></li>
<li><a href="#system-architecture" id="toc-system-architecture">4.
System Architecture</a>
<ul>
<li><a href="#overview" id="toc-overview">4.1 Overview</a></li>
<li><a href="#processing-pipeline" id="toc-processing-pipeline">4.2
Processing Pipeline</a></li>
<li><a href="#data-flow" id="toc-data-flow">4.3 Data Flow</a></li>
<li><a href="#software-architecture" id="toc-software-architecture">4.4
Software Architecture</a></li>
</ul></li>
<li><a href="#hardware-components" id="toc-hardware-components">5.
Hardware Components</a>
<ul>
<li><a href="#prototype-camera-active-silicon-harrier-10x-af-zoom"
id="toc-prototype-camera-active-silicon-harrier-10x-af-zoom">5.1
Prototype Camera: Active Silicon Harrier 10x AF-Zoom</a></li>
<li><a href="#prototype-imu-orange-cube-flight-controller"
id="toc-prototype-imu-orange-cube-flight-controller">5.2 Prototype IMU:
Orange Cube Flight Controller</a></li>
<li><a href="#silhil-validation-environment-stellarium"
id="toc-silhil-validation-environment-stellarium">5.3 SIL/HIL Validation
Environment: Stellarium</a></li>
<li><a href="#alternative-configuration-a-gopro-hero-7-black"
id="toc-alternative-configuration-a-gopro-hero-7-black">5.4 Alternative
Configuration A: GoPro Hero 7 Black</a></li>
<li><a href="#alternative-configuration-b-zwo-asi585mc-entaniya-m12-220"
id="toc-alternative-configuration-b-zwo-asi585mc-entaniya-m12-220">5.5
Alternative Configuration B: ZWO ASI585MC + Entaniya M12 220</a></li>
<li><a href="#hardware-configuration-comparison"
id="toc-hardware-configuration-comparison">5.6 Hardware Configuration
Comparison</a></li>
<li><a href="#cost-breakdown-by-configuration"
id="toc-cost-breakdown-by-configuration">5.7 Cost Breakdown by
Configuration</a></li>
</ul></li>
<li><a href="#camera-imu-sensor-fusion"
id="toc-camera-imu-sensor-fusion">6. Camera + IMU Sensor Fusion</a>
<ul>
<li><a href="#fundamental-concept" id="toc-fundamental-concept">6.1
Fundamental Concept</a></li>
<li><a href="#physical-setup-rigid-camera-imu-coupling"
id="toc-physical-setup-rigid-camera-imu-coupling">6.2 Physical Setup:
Rigid Camera-IMU Coupling</a></li>
<li><a href="#coordinate-systems-and-transformations"
id="toc-coordinate-systems-and-transformations">6.3 Coordinate Systems
and Transformations</a></li>
<li><a href="#d-celestial-sphere-visualization"
id="toc-d-celestial-sphere-visualization">6.4 3D Celestial Sphere
Visualization</a></li>
<li><a href="#imu-camera-calibration"
id="toc-imu-camera-calibration">6.5 IMU-Camera Calibration</a></li>
<li><a href="#sensor-fusion-algorithms"
id="toc-sensor-fusion-algorithms">6.6 Sensor Fusion Algorithms</a></li>
<li><a href="#benefits-for-star-tracking"
id="toc-benefits-for-star-tracking">6.7 Benefits for Star
Tracking</a></li>
<li><a href="#video-demonstration" id="toc-video-demonstration">6.8
Video Demonstration</a></li>
<li><a href="#hybrid-stabilization-gyroscope-template-matching"
id="toc-hybrid-stabilization-gyroscope-template-matching">6.9 Hybrid
Stabilization: Gyroscope + Template Matching</a></li>
<li><a href="#simple-star-solver-proof-of-concept"
id="toc-simple-star-solver-proof-of-concept">6.10 Simple Star Solver
(Proof of Concept)</a></li>
</ul></li>
<li><a href="#imu-based-motion-deblur"
id="toc-imu-based-motion-deblur">7. IMU-Based Motion Deblur</a>
<ul>
<li><a href="#the-motion-blur-problem"
id="toc-the-motion-blur-problem">7.1 The Motion Blur Problem</a></li>
<li><a href="#psf-generation-from-imu-data"
id="toc-psf-generation-from-imu-data">8.2 PSF Generation from IMU
Data</a></li>
<li><a href="#deconvolution-algorithms"
id="toc-deconvolution-algorithms">8.3 Deconvolution Algorithms</a></li>
<li><a href="#the-star-trail-overlap-problem"
id="toc-the-star-trail-overlap-problem">8.4 The Star Trail Overlap
Problem</a></li>
<li><a href="#results-motion-deblur-performance"
id="toc-results-motion-deblur-performance">8.5 Results: Motion Deblur
Performance</a></li>
<li><a href="#implementation" id="toc-implementation">8.6
Implementation</a></li>
<li><a href="#practical-considerations"
id="toc-practical-considerations">7.7 Practical Considerations</a></li>
</ul></li>
<li><a href="#software-algorithms" id="toc-software-algorithms">8.
Software Algorithms</a>
<ul>
<li><a href="#gyroscope-data-processing"
id="toc-gyroscope-data-processing">8.1 Gyroscope Data
Processing</a></li>
<li><a href="#motion-compensation" id="toc-motion-compensation">8.2
Motion Compensation</a></li>
<li><a href="#star-detection" id="toc-star-detection">8.3 Star
Detection</a></li>
<li><a href="#triangle-based-star-matching"
id="toc-triangle-based-star-matching">8.4 Triangle-Based Star
Matching</a></li>
<li><a href="#quality-assessment" id="toc-quality-assessment">8.5
Quality Assessment</a></li>
<li><a href="#image-stacking" id="toc-image-stacking">8.6 Image
Stacking</a></li>
</ul></li>
<li><a href="#performance-analysis" id="toc-performance-analysis">9.
Performance Analysis</a>
<ul>
<li><a href="#signal-to-noise-ratio-improvement"
id="toc-signal-to-noise-ratio-improvement">9.1 Signal-to-Noise Ratio
Improvement</a></li>
<li><a href="#limiting-magnitude" id="toc-limiting-magnitude">9.2
Limiting Magnitude</a></li>
<li><a href="#angular-resolution" id="toc-angular-resolution">9.3
Angular Resolution</a></li>
<li><a href="#processing-performance"
id="toc-processing-performance">9.4 Processing Performance</a></li>
<li><a href="#accuracy-metrics" id="toc-accuracy-metrics">9.5 Accuracy
Metrics</a></li>
<li><a href="#vibration-attenuation-performance"
id="toc-vibration-attenuation-performance">9.6 Vibration Attenuation
Performance</a></li>
</ul></li>
<li><a href="#cost-comparison" id="toc-cost-comparison">10. Cost
Comparison</a>
<ul>
<li><a href="#our-low-cost-systems" id="toc-our-low-cost-systems">10.1
Our Low-Cost Systems</a></li>
<li><a href="#comparison-with-commercial-solutions"
id="toc-comparison-with-commercial-solutions">10.2 Comparison with
Commercial Solutions</a></li>
<li><a href="#cost-effectiveness-ratio"
id="toc-cost-effectiveness-ratio">9.3 Cost-Effectiveness Ratio</a></li>
<li><a href="#value-proposition" id="toc-value-proposition">9.4 Value
Proposition</a></li>
</ul></li>
<li><a href="#results-and-discussion"
id="toc-results-and-discussion">11. Results and Discussion</a>
<ul>
<li><a href="#phase-1-validation-results"
id="toc-phase-1-validation-results">11.1 Phase 1 Validation
Results</a></li>
<li><a href="#phase-2-algorithm-enhancement-results"
id="toc-phase-2-algorithm-enhancement-results">11.2 Phase 2 Algorithm
Enhancement Results</a></li>
<li><a href="#advantages-of-our-approach"
id="toc-advantages-of-our-approach">11.4 Advantages of Our
Approach</a></li>
<li><a href="#limitations" id="toc-limitations">11.3
Limitations</a></li>
<li><a href="#comparison-with-mechanical-tracking"
id="toc-comparison-with-mechanical-tracking">11.4 Comparison with
Mechanical Tracking</a></li>
<li><a href="#suitable-applications" id="toc-suitable-applications">11.5
Suitable Applications</a></li>
<li><a href="#software-in-the-loop-testing-with-stellarium"
id="toc-software-in-the-loop-testing-with-stellarium">11.6
Software-in-the-Loop Testing with Stellarium</a></li>
</ul></li>
<li><a href="#conclusions-and-future-work"
id="toc-conclusions-and-future-work">12. Conclusions and Future Work</a>
<ul>
<li><a href="#conclusions" id="toc-conclusions">12.1
Conclusions</a></li>
<li><a href="#future-work" id="toc-future-work">12.2 Future
Work</a></li>
</ul></li>
<li><a href="#references" id="toc-references">13. References</a>
<ul>
<li><a href="#academic-literature" id="toc-academic-literature">Academic
Literature</a></li>
<li><a href="#technical-references"
id="toc-technical-references">Technical References</a></li>
<li><a href="#online-resources" id="toc-online-resources">Online
Resources</a></li>
<li><a href="#hardware-references" id="toc-hardware-references">Hardware
References</a></li>
<li><a href="#sensor-fusion-references"
id="toc-sensor-fusion-references">Sensor Fusion References</a></li>
<li><a href="#motion-deblur-references"
id="toc-motion-deblur-references">Motion Deblur References</a></li>
</ul></li>
<li><a href="#appendix-a-system-requirements"
id="toc-appendix-a-system-requirements">Appendix A: System
Requirements</a>
<ul>
<li><a href="#a.1-prototype-harrier-10x-orange-cube-validated"
id="toc-a.1-prototype-harrier-10x-orange-cube-validated">A.1 Prototype:
Harrier 10x + Orange Cube (Validated)</a></li>
<li><a href="#a.2-alternative-a-gopro-based-system"
id="toc-a.2-alternative-a-gopro-based-system">A.2 Alternative A:
GoPro-Based System</a></li>
<li><a href="#a.3-alternative-b-asi585mc-entaniya-all-sky-system"
id="toc-a.3-alternative-b-asi585mc-entaniya-all-sky-system">A.3
Alternative B: ASI585MC + Entaniya All-Sky System</a></li>
<li><a href="#a.4-software-dependencies"
id="toc-a.4-software-dependencies">A.4 Software Dependencies</a></li>
</ul></li>
<li><a href="#appendix-b-quick-start-guide"
id="toc-appendix-b-quick-start-guide">Appendix B: Quick Start Guide</a>
<ul>
<li><a href="#b.1-installation" id="toc-b.1-installation">B.1
Installation</a></li>
<li><a href="#b.2-basic-usage" id="toc-b.2-basic-usage">B.2 Basic
Usage</a></li>
<li><a href="#b.3-configuration-file"
id="toc-b.3-configuration-file">B.3 Configuration File</a></li>
</ul></li>
<li><a href="#appendix-c-glossary" id="toc-appendix-c-glossary">Appendix
C: Glossary</a></li>
<li><a href="#changelog" id="toc-changelog">Changelog</a>
<ul>
<li><a href="#version-1.6-january-2026"
id="toc-version-1.6-january-2026">Version 1.6 (January 2026)</a></li>
<li><a href="#version-1.2-january-2026"
id="toc-version-1.2-january-2026">Version 1.2 (January 2026)</a></li>
<li><a href="#version-1.1-january-2026"
id="toc-version-1.1-january-2026">Version 1.1 (January 2026)</a></li>
<li><a href="#version-1.0-january-2026"
id="toc-version-1.0-january-2026">Version 1.0 (January 2026)</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
<h1
id="low-cost-star-tracker-a-software-based-approach-using-consumer-grade-hardware">Low-Cost
Star Tracker: A Software-Based Approach Using Consumer-Grade
Hardware</h1>
<h2 id="project-report">Project Report</h2>
<p><strong>Authors:</strong> Giray Yillikci <strong>Version:</strong>
1.6 <strong>Date:</strong> January 2026</p>
<hr />
<h2 id="abstract">Abstract</h2>
<p>This paper presents a novel low-cost star tracker system designed to
capture faint starlight even in heavily light-polluted urban
environments. The core approach combines gyroscope-based motion
compensation with video frame stacking: by first stabilizing video
captured from moving platforms—seaborne, airborne, and land-based
vehicles—using high-rate IMU data, then aligning and stacking hundreds
of frames, we effectively increase the signal-to-noise ratio to reveal
stars that would otherwise be invisible in individual frames. Our
hardware configuration centers on the Active Silicon Harrier 10x AF-Zoom
Camera, featuring a 1/2.8” Sony IMX462LQR-C CMOS sensor with exceptional
low-light performance (0.0004 lux minimum illumination in monochrome
mode), 10x optical zoom (f=5.1-51mm, F1.6-1.8), and 1080p60 video output
via simultaneous USB 3.1 and HDMI interfaces. The camera is paired with
an Orange Cube flight controller (featuring the ICM-20948 9-axis IMU)
for high-precision gyroscope-based motion compensation. The system is
validated using Stellarium planetarium software to generate
high-fidelity synthetic star fields displayed on a monitor, providing a
controlled SIL/HIL test environment with configurable camera motion and
vibration profiles. By implementing sophisticated frame stacking
techniques and star detection algorithms, our system achieves a cost
reduction of 95-99% compared to commercial star trackers while
maintaining acceptable performance for amateur astrophotography, meteor
detection, and educational applications. The complete system can be
assembled for $500-1,500, compared to $10,000-$500,000 for commercial
alternatives.</p>
<p><strong>Keywords:</strong> Star Tracker, Astrophotography, Gyroscope
Stabilization, Image Stacking, Low-Cost Sensors, Harrier Camera, Orange
Cube, ICM-20948 IMU, Stellarium, Motion Compensation, SIL/HIL
Testing</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol type="1">
<li><a href="#1-introduction">Introduction</a></li>
<li><a href="#2-literature-review">Literature Review</a></li>
<li><a href="#3-commercial-star-tracker-analysis">Commercial Star
Tracker Analysis</a></li>
<li><a href="#4-system-architecture">System Architecture</a></li>
<li><a href="#5-hardware-components">Hardware Components</a></li>
<li><a href="#6-camera--imu-sensor-fusion">Camera + IMU Sensor
Fusion</a></li>
<li><a href="#7-imu-based-motion-deblur">IMU-Based Motion
Deblur</a></li>
<li><a href="#8-software-algorithms">Software Algorithms</a></li>
<li><a href="#9-performance-analysis">Performance Analysis</a></li>
<li><a href="#10-cost-comparison">Cost Comparison</a></li>
<li><a href="#11-results-and-discussion">Results and Discussion</a></li>
<li><a href="#12-conclusions-and-future-work">Conclusions and Future
Work</a></li>
<li><a href="#13-references">References</a></li>
</ol>
<hr />
<h2 id="introduction">1. Introduction</h2>
<h3 id="background">1.1 Background</h3>
<p>Star trackers are optical instruments that identify stars in an image
and determine the orientation of a spacecraft or camera relative to the
celestial sphere. They are essential components in spacecraft attitude
determination systems, astronomical observations, and astrophotography.
Traditional star trackers are precision instruments with costs ranging
from tens of thousands to hundreds of thousands of dollars, making them
inaccessible to amateur astronomers, educational institutions, and
researchers with limited budgets.</p>
<h3 id="problem-statement">1.2 Problem Statement</h3>
<p>The high cost of commercial star trackers creates significant
barriers for: - Amateur astronomers seeking quality astrophotography -
Educational institutions teaching astronomy and space sciences -
Researchers in developing countries - CubeSat and small satellite
projects with limited budgets - Citizen science initiatives</p>
<h3 id="proposed-solution">1.3 Proposed Solution</h3>
<p>We present a software-intensive approach that shifts complexity from
expensive hardware to sophisticated algorithms. The core concept is to
capture faint starlight even in light-polluted environments by combining
gyroscope-based motion compensation with video frame
stacking—stabilizing video from moving platforms (seaborne, airborne,
and land-based vehicles), then aligning and stacking hundreds of frames
to increase the signal-to-noise ratio and reveal stars invisible in
individual frames.</p>
<p><a href="https://www.youtube.com/shorts/uqEzUAQ2iE8"><img
src="https://img.youtube.com/vi/uqEzUAQ2iE8/0.jpg"
alt="Video Demonstration" /></a></p>
<p><em>Click thumbnail to watch video demonstration</em></p>
<p><strong>Prototype System:</strong> Our validated prototype uses the
Active Silicon Harrier 10x AF-Zoom Camera paired with an Orange Cube
flight controller (ICM-20948 9-axis IMU). The system is tested using
Stellarium planetarium software to generate high-fidelity synthetic star
fields, providing a controlled SIL/HIL environment with configurable
motion and vibration profiles.</p>
<p><strong>Alternative Hardware:</strong> The software pipeline is
designed to work with various camera+IMU combinations, including
consumer options like the GoPro Hero 7 Black ($200-400) with its
embedded gyroscope, or dedicated astronomy cameras like the ZWO ASI585MC
with external IMU.</p>
<h3 id="contributions">1.4 Contributions</h3>
<p>This work makes the following contributions: 1. A complete
open-source star tracker pipeline using consumer hardware 2. Novel
gyroscope-based motion compensation using quaternion mathematics 3.
Adaptive frame stacking with quality-based selection 4. Triangle-based
star matching algorithm for robust alignment 5. Comprehensive
cost-benefit analysis compared to commercial solutions</p>
<hr />
<h2 id="literature-review">2. Literature Review</h2>
<h3 id="traditional-star-tracker-technology">2.1 Traditional Star
Tracker Technology</h3>
<p>Star trackers have evolved significantly since their inception in the
1960s. Early systems used photomultiplier tubes and mechanical scanning,
while modern systems employ CCD and CMOS sensors with sophisticated
pattern recognition algorithms.</p>
<h4 id="lost-in-space-algorithms">2.1.1 Lost-in-Space Algorithms</h4>
<p>The fundamental challenge in star tracking is the “Lost-in-Space”
(LIS) problem—identifying stars without prior attitude knowledge.
Several algorithms have been developed:</p>
<p><strong>Grid Algorithm (Padgett &amp; Kreutz-Delgado, 1997)</strong>
Divides the celestial sphere into a grid and uses lookup tables for
rapid identification. Computational complexity: O(n²) where n is the
number of detected stars.</p>
<p><strong>Pyramid Algorithm (Mortari et al., 2004)</strong> Uses
four-star patterns forming pyramids for robust identification. Offers
improved reliability but higher computational cost.</p>
<p><strong>Geometric Voting (Kolomenkin et al., 2008)</strong> Employs a
voting scheme based on angular distances between star pairs. Provides
good balance between speed and reliability.</p>
<p><strong>Triangle Algorithm (Liebe, 1993)</strong> Matches triangular
patterns formed by star triplets. The basis for our implementation due
to its rotation and scale invariance.</p>
<h4 id="centroiding-techniques">2.1.2 Centroiding Techniques</h4>
<p>Accurate star position determination requires sub-pixel
centroiding:</p>
<ul>
<li><strong>Center of Gravity (CoG):</strong> Simple weighted average,
accuracy ~0.1 pixels</li>
<li><strong>Gaussian Fitting:</strong> Models star PSF as 2D Gaussian,
accuracy ~0.05 pixels</li>
<li><strong>Iterative Weighted Centroiding (IWC):</strong> Iteratively
refines weights, accuracy ~0.02 pixels</li>
</ul>
<h3 id="gyroscope-based-stabilization">2.2 Gyroscope-Based
Stabilization</h3>
<p>The use of gyroscopes for image stabilization has been extensively
studied in both consumer electronics and aerospace applications.</p>
<h4 id="mems-gyroscopes">2.2.1 MEMS Gyroscopes</h4>
<p>Micro-Electro-Mechanical Systems (MEMS) gyroscopes have
revolutionized motion sensing by providing compact, low-cost angular
rate measurements. Modern action cameras like the GoPro Hero series
incorporate 3-axis MEMS gyroscopes with: - Sampling rates: 200-400 Hz -
Noise density: 0.005-0.01 °/s/√Hz - Bias stability: 1-10 °/hr</p>
<h4 id="sensor-fusion">2.2.2 Sensor Fusion</h4>
<p>Combining gyroscope data with accelerometer and magnetometer readings
(9-DOF fusion) improves orientation estimation. Common fusion algorithms
include: - Complementary filters - Kalman filters (Extended and
Unscented variants) - Madgwick filter - Mahony filter</p>
<h4 id="gyroflow-project">2.2.3 Gyroflow Project</h4>
<p>The open-source Gyroflow project (gyroflow.xyz) demonstrates the
effectiveness of gyroscope-based video stabilization for action cameras.
Our work extends these concepts specifically for astrophotography
applications.</p>
<h3 id="image-stacking-techniques">2.3 Image Stacking Techniques</h3>
<p>Image stacking is fundamental to astrophotography, improving
signal-to-noise ratio (SNR) by combining multiple exposures.</p>
<h4 id="stacking-methods">2.3.1 Stacking Methods</h4>
<p><strong>Mean Stacking</strong> Simple averaging of aligned frames.
SNR improvement: √n where n is frame count. Sensitive to outliers
(satellites, cosmic rays).</p>
<p><strong>Median Stacking</strong> Uses median value at each pixel.
Robust to outliers but discards valid signal. SNR improvement:
~0.8√n.</p>
<p><strong>Sigma-Clipping</strong> Iteratively rejects pixels deviating
more than kσ from the mean. Balances outlier rejection with signal
preservation.</p>
<p><strong>Winsorized Mean</strong> Clips extreme values to specified
percentiles before averaging. Computationally efficient approximation of
sigma-clipping.</p>
<h4 id="frame-alignment">2.3.2 Frame Alignment</h4>
<p>Accurate frame registration is critical for effective stacking: -
<strong>Phase Correlation:</strong> FFT-based translation detection -
<strong>Feature Matching:</strong> SIFT, ORB, or star-based keypoints -
<strong>Optical Flow:</strong> Dense motion estimation between
frames</p>
<h3 id="low-cost-star-tracker-initiatives">2.4 Low-Cost Star Tracker
Initiatives</h3>
<p>Several research groups have explored affordable star tracker
alternatives:</p>
<h4 id="cubesat-star-trackers">2.4.1 CubeSat Star Trackers</h4>
<p><strong>ST-16 (Sinclair Interplanetary)</strong> Commercial CubeSat
star tracker, ~$50,000, 2 arcsec accuracy.</p>
<p><strong>NST-1 (Naval Postgraduate School)</strong> Academic
development, ~$5,000 in components, 30 arcsec accuracy.</p>
<h4 id="smartphone-based-systems">2.4.2 Smartphone-Based Systems</h4>
<p>Research by Rijlaarsdam et al. (2020) demonstrated star tracking
using smartphone cameras, achieving 0.05° accuracy in controlled
conditions.</p>
<h4 id="raspberry-pi-systems">2.4.3 Raspberry Pi Systems</h4>
<p>Multiple hobbyist projects use Raspberry Pi with camera modules (HQ
Camera, ~$50) for basic star tracking, though without integrated
gyroscope stabilization.</p>
<h3 id="consumer-camera-astrophotography">2.5 Consumer Camera
Astrophotography</h3>
<p>Consumer cameras have gained popularity in the amateur astronomy
community as accessible alternatives to dedicated astronomy
equipment.</p>
<p><strong>GoPro Action Cameras:</strong> The GoPro Hero series offers
built-in gyroscopes and GPMF telemetry, making them attractive for
motion-compensated astrophotography: - Night Lapse mode enables
long-exposure sequences - Wide-angle lenses capture large star fields -
Built-in IMU enables gyro-based stabilization - Raw format preserves
maximum dynamic range</p>
<p><strong>Industrial/Machine Vision Cameras:</strong> Cameras like the
Active Silicon Harrier series offer superior low-light performance
(0.0004 lux) and flexible integration with external IMUs for demanding
applications on moving platforms.</p>
<p><strong>Astronomy Cameras:</strong> Dedicated cameras like the ZWO
ASI585MC offer cooled sensors and exceptional sensitivity but require
external IMU integration for motion compensation.</p>
<hr />
<h2 id="commercial-star-tracker-analysis">3. Commercial Star Tracker
Analysis</h2>
<h3 id="market-overview">3.1 Market Overview</h3>
<p>The star tracker market is segmented by application, accuracy, and
form factor:</p>
<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 27%" />
<col style="width: 21%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr>
<th>Segment</th>
<th>Typical Cost</th>
<th>Accuracy</th>
<th>Primary Users</th>
</tr>
</thead>
<tbody>
<tr>
<td>Spacecraft Grade</td>
<td>$100,000-$500,000</td>
<td>1-10 arcsec</td>
<td>Space agencies, satellite manufacturers</td>
</tr>
<tr>
<td>CubeSat Grade</td>
<td>$20,000-$100,000</td>
<td>10-60 arcsec</td>
<td>University research, small satellites</td>
</tr>
<tr>
<td>Commercial Astronomy</td>
<td>$5,000-$30,000</td>
<td>1-5 arcmin</td>
<td>Professional observatories</td>
</tr>
<tr>
<td>Amateur Grade</td>
<td>$200-$2,000</td>
<td>5-30 arcmin</td>
<td>Amateur astronomers</td>
</tr>
</tbody>
</table>
<h3 id="commercial-products-analysis">3.2 Commercial Products
Analysis</h3>
<h4 id="spacecraft-star-trackers">3.2.1 Spacecraft Star Trackers</h4>
<p><strong>Ball Aerospace CT-2020</strong> - Accuracy: 2 arcsec
(pitch/yaw), 15 arcsec (roll) - Update rate: 10 Hz - Mass: 2.5 kg -
Power: 10 W - Cost: ~$300,000</p>
<p><strong>Sodern Hydra</strong> - Accuracy: 1 arcsec (pitch/yaw) -
Update rate: 4 Hz - Mass: 2.3 kg - Cost: ~$400,000</p>
<p><strong>Terma T1/T2</strong> - Accuracy: 5 arcsec - Mass: 0.5-1.5 kg
- Cost: ~$150,000</p>
<h4 id="cubesat-star-trackers-1">3.2.2 CubeSat Star Trackers</h4>
<p><strong>Blue Canyon Technologies NST</strong> - Accuracy: 6 arcsec
cross-boresight - Mass: 350g - Power: 1.5 W - Cost: ~$75,000</p>
<p><strong>Berlin Space Technologies ST200</strong> - Accuracy: 30
arcsec - Mass: 250g - Cost: ~$40,000</p>
<p><strong>Sinclair Interplanetary ST-16RT2</strong> - Accuracy: 2-7
arcsec - Mass: 185g - Cost: ~$50,000</p>
<h4 id="amateur-astronomy-mounts">3.2.3 Amateur Astronomy Mounts</h4>
<p><strong>Sky-Watcher Star Adventurer 2i</strong> - Type: Portable
equatorial mount - Tracking accuracy: ±5 arcmin/hr - Payload: 5 kg -
Cost: ~$400</p>
<p><strong>iOptron SkyGuider Pro</strong> - Tracking accuracy: ±3.5
arcmin/hr - Payload: 5 kg - Cost: ~$500</p>
<p><strong>Celestron CGEM II</strong> - Tracking accuracy: ±3 arcmin RMS
- Payload: 18 kg - Cost: ~$2,000</p>
<h3 id="cost-breakdown-analysis">3.3 Cost Breakdown Analysis</h3>
<p>Commercial star tracker costs are driven by:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>% of Total Cost</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>Optics</td>
<td>15-25%</td>
<td>Precision ground lenses, low distortion</td>
</tr>
<tr>
<td>Sensor</td>
<td>10-20%</td>
<td>Space-qualified, radiation-hardened CCDs</td>
</tr>
<tr>
<td>Processing</td>
<td>10-15%</td>
<td>Rad-hard FPGAs/processors</td>
</tr>
<tr>
<td>Calibration</td>
<td>20-30%</td>
<td>Extensive ground testing, thermal-vacuum</td>
</tr>
<tr>
<td>Qualification</td>
<td>15-25%</td>
<td>Space environment testing</td>
</tr>
<tr>
<td>Development</td>
<td>10-15%</td>
<td>R&amp;D amortization</td>
</tr>
</tbody>
</table>
<h3 id="barrier-to-entry">3.4 Barrier to Entry</h3>
<p>The high costs of commercial star trackers stem from:</p>
<ol type="1">
<li><strong>Radiation Hardening:</strong> Space-grade components must
withstand cosmic radiation</li>
<li><strong>Thermal Stability:</strong> Wide operating temperature
ranges (-40°C to +60°C)</li>
<li><strong>Reliability Requirements:</strong> Mean Time Between
Failures (MTBF) &gt; 100,000 hours</li>
<li><strong>Calibration Costs:</strong> Each unit requires individual
calibration</li>
<li><strong>Low Volume Production:</strong> Limited market size prevents
economies of scale</li>
</ol>
<hr />
<h2 id="system-architecture">4. System Architecture</h2>
<h3 id="overview">4.1 Overview</h3>
<p>Our low-cost star tracker employs a software-intensive architecture
that maximizes the use of consumer hardware while implementing
sophisticated algorithms in software.</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────────┐
│                        LOW-COST STAR TRACKER SYSTEM                      │
│                           (Prototype Configuration)                      │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────────────────┐  │
│  │  Harrier 10x │    │   Moving     │    │      Processing          │  │
│  │  AF-Zoom     │───▶│  Platform    │───▶│       Computer           │  │
│  │   Camera     │    │ (land/sea/air)│   │   (Python Pipeline)      │  │
│  └──────────────┘    └──────────────┘    └──────────────────────────┘  │
│         │                   │                        │                   │
│         ▼                   ▼                        ▼                   │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────────────────┐  │
│  │ Video Stream │    │ Orange Cube  │    │     Output Image         │  │
│  │ (USB/HDMI)   │    │ IMU Data     │    │   (Stacked, Aligned)     │  │
│  └──────────────┘    └──────────────┘    └──────────────────────────┘  │
│                                                                          │
│  SIL/HIL Validation: Stellarium synthetic star fields on monitor        │
└─────────────────────────────────────────────────────────────────────────┘</code></pre>
<h3 id="processing-pipeline">4.2 Processing Pipeline</h3>
<p>The system processes data through eight sequential stages:</p>
<pre><code>┌─────────────┐   ┌─────────────┐   ┌─────────────┐   ┌─────────────┐
│    Stage 1  │   │   Stage 2   │   │   Stage 3   │   │   Stage 4   │
│    Gyro     │──▶│   Motion    │──▶│    Frame    │──▶│    Star     │
│  Extraction │   │Compensation │   │ Extraction  │   │  Detection  │
└─────────────┘   └─────────────┘   └─────────────┘   └─────────────┘
                                                              │
                                                              ▼
┌─────────────┐   ┌─────────────┐   ┌─────────────┐   ┌─────────────┐
│   Stage 8   │   │   Stage 7   │   │   Stage 6   │   │   Stage 5   │
│   Output    │◀──│   Image     │◀──│    Frame    │◀──│   Quality   │
│   Saving    │   │  Stacking   │   │  Alignment  │   │ Assessment  │
└─────────────┘   └─────────────┘   └─────────────┘   └─────────────┘</code></pre>
<h3 id="data-flow">4.3 Data Flow</h3>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 18%" />
<col style="width: 21%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr>
<th>Stage</th>
<th>Input</th>
<th>Output</th>
<th>Key Algorithm</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. Gyro Extraction</td>
<td>MP4 video file</td>
<td>GyroData (quaternions)</td>
<td>GPMF parsing, RK4 integration</td>
</tr>
<tr>
<td>2. Motion Compensation</td>
<td>Frames + quaternions</td>
<td>Stabilized frames</td>
<td>Homography transformation</td>
</tr>
<tr>
<td>3. Frame Extraction</td>
<td>Video stream</td>
<td>Individual frames</td>
<td>FFmpeg/OpenCV decode</td>
</tr>
<tr>
<td>4. Star Detection</td>
<td>Frame images</td>
<td>StarField objects</td>
<td>Connected component analysis</td>
</tr>
<tr>
<td>5. Quality Assessment</td>
<td>StarFields</td>
<td>Quality scores</td>
<td>Multi-factor scoring</td>
</tr>
<tr>
<td>6. Frame Alignment</td>
<td>Quality frames</td>
<td>Aligned frames</td>
<td>Triangle matching + RANSAC</td>
</tr>
<tr>
<td>7. Image Stacking</td>
<td>Aligned frames</td>
<td>Stacked image</td>
<td>Sigma-clipping</td>
</tr>
<tr>
<td>8. Output Saving</td>
<td>Stacked image</td>
<td>TIFF/FITS file</td>
<td>16-bit encoding</td>
</tr>
</tbody>
</table>
<h3 id="software-architecture">4.4 Software Architecture</h3>
<pre><code>lowcost-star-tracker/
├── pyproject.toml                    # Project configuration
├── README.md                         # Main documentation
│
├── Root-level scripts/
│   ├── compare_stabilization.py      # Stabilization comparison tool
│   ├── compare_videos.py             # Video comparison utility
│   ├── convert_md_to_docx.py         # Documentation converter
│   ├── debug_stabilization.py        # Stabilization debugging
│   ├── gyro_stabilizer.py            # Gyroscope stabilization
│   ├── live_simple_star_solve.py     # Simple star solving (live)
│   ├── live_tetra3_solve.py          # Tetra3 star solving (live)
│   ├── plot_gyro.py                  # Gyroscope data visualization
│   ├── stabilize_video.py            # Video stabilization tool
│   ├── stellarium_config.py          # Stellarium configuration
│   ├── stellarium_shake.py           # Stellarium shake simulator
│   ├── stellarium_toggle_labels.py   # Stellarium UI control
│   └── test_witmotion_pywitmotion.py # IMU testing
│
├── src/
│   ├── algorithms/                   # Core algorithms
│   ├── calibration/                  # Calibration modules
│   ├── plate_solving/                # Plate solving algorithms
│   └── star_tracker/                 # Main star tracker package
│
├── camera/                           # Camera and visualization tools
│   ├── celestial_sphere_3d.py        # 3D celestial sphere visualization
│   ├── celestial_sphere_viewer.py    # Celestial sphere viewer
│   ├── integrated_stabilizer.py      # Integrated stabilization system
│   └── usb_camera_viewer.py          # USB camera interface
│
├── calibration/                      # Calibration data and scripts
│
├── imu/                              # IMU integration
│   ├── __init__.py                   # IMU package initialization
│   ├── find_witmotion_windows.py     # Windows IMU detection
│   ├── pywitmotion_adapter.py        # Pywitmotion adapter
│   └── witmotion_reader.py           # Witmotion IMU reader
│
├── mavlink/                          # MAVLink integration
│   └── orange_cube_reader.py         # Orange Cube flight controller
│
├── wfb-stabilizer/                   # WFB stabilizer variants
│   ├── README.md                     # WFB documentation
│   ├── ejo_wfb_stabilizer.py         # EJO WFB stabilizer
│   └── run_camera1_*.py              # Various camera stabilizer configs
│
├── validation/                       # Validation framework
│   ├── VALIDATION_REPORT.md          # Validation report
│   ├── generate_validation_plots.py  # Plot generation
│   └── validation_framework.py       # Validation framework
│
├── experiments/                      # Experimental code
├── motion_deblur/                    # Motion deblur algorithms
├── output/                           # Output files
│
├── external/                         # External dependencies
│   ├── pywitmotion/                  # Pywitmotion library
│   └── tetra3/                       # Tetra3 star matching
│
├── data/
│   └── lens_profiles/                # Camera calibration profiles
│
├── examples/                         # Example videos and data
│
└── docs/                             # Documentation
    ├── Development_Roadmap.md        # Project roadmap
    ├── LowCost_StarTracker_Technical_Paper.md  # Technical paper
    └── images/                       # Documentation images</code></pre>
<hr />
<h2 id="hardware-components">5. Hardware Components</h2>
<blockquote>
<p><strong>Hardware Configurations:</strong></p>
<p><strong>Prototype System (validated):</strong> -
<strong>Camera</strong>: Active Silicon Harrier 10x AF-Zoom (1080p60,
0.0004 lux sensitivity) - <strong>IMU</strong>: Orange Cube flight
controller with ICM-20948 9-axis IMU (MAVLink telemetry) -
<strong>Validation</strong>: Stellarium planetarium software generating
synthetic star fields - <strong>Target Platforms</strong>: Seaborne,
airborne, and land-based vehicles</p>
<p><strong>Alternative Configurations (software-compatible):</strong> -
<strong>Configuration A</strong>: GoPro Hero 7 Black (embedded
gyroscope, GPMF telemetry) - <strong>Configuration B</strong>: ZWO
ASI585MC + Entaniya M12 220 + external IMU (all-sky monitoring)</p>
</blockquote>
<h3 id="prototype-camera-active-silicon-harrier-10x-af-zoom">5.1
Prototype Camera: Active Silicon Harrier 10x AF-Zoom</h3>
<p>The Active Silicon Harrier 10x AF-Zoom camera serves as the primary
sensor platform for our validated prototype, selected for its
exceptional low-light performance and flexible integration capabilities
for moving platform applications.</p>
<h4 id="imaging-specifications">5.1.1 Imaging Specifications</h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sensor Type</td>
<td>1/2.8” Sony IMX462LQR-C CMOS (progressive scan)</td>
</tr>
<tr>
<td>Effective Pixels</td>
<td>~2.13 MP</td>
</tr>
<tr>
<td>Horizontal Resolution</td>
<td>700 TV lines</td>
</tr>
<tr>
<td>Optical Zoom</td>
<td>10x (f=5.1mm to 51mm, F1.6 to 1.8)</td>
</tr>
<tr>
<td>Digital Zoom</td>
<td>2x to 32x</td>
</tr>
<tr>
<td>Field of View</td>
<td>57° (wide) to 6.2° (telephoto)</td>
</tr>
<tr>
<td>Video Output</td>
<td>1080p60, 1080p30, 720p60, 720p30</td>
</tr>
<tr>
<td>Interfaces</td>
<td>Simultaneous USB 3.1 Gen1 (UVC) + HDMI</td>
</tr>
<tr>
<td>Minimum Illumination</td>
<td><strong>0.003 lux (color), 0.0004 lux (mono)</strong></td>
</tr>
<tr>
<td>Shutter Speed</td>
<td>1/30 to 1/30,000 sec</td>
</tr>
<tr>
<td>S/N Ratio</td>
<td>&gt;50 dB</td>
</tr>
</tbody>
</table>
<h4 id="physical-characteristics">5.1.2 Physical Characteristics</h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dimensions</td>
<td>62.4mm × 56mm × 45.5mm</td>
</tr>
<tr>
<td>Weight</td>
<td>138g</td>
</tr>
<tr>
<td>Power</td>
<td>9-15V DC, ~4.8W @ 1080p60</td>
</tr>
<tr>
<td>Control Protocols</td>
<td>UVC, VISCA (USB/TTL/RS-232), Pelco D/P</td>
</tr>
<tr>
<td>Minimum Object Distance</td>
<td>100mm to 1000mm</td>
</tr>
</tbody>
</table>
<h4 id="advantages-for-star-tracking-on-moving-platforms">5.1.3
Advantages for Star Tracking on Moving Platforms</h4>
<ul>
<li><strong>Exceptional low-light sensitivity:</strong> 0.0004 lux
enables star detection even in challenging conditions</li>
<li><strong>10x optical zoom:</strong> Allows flexible field-of-view
selection without lens changes</li>
<li><strong>Fast autofocus:</strong> Maintains sharp focus during
platform motion</li>
<li><strong>Dual output:</strong> Simultaneous USB and HDMI enables
recording + live preview</li>
<li><strong>Compact form factor:</strong> Suitable for integration on
vehicles, drones, and marine vessels</li>
<li><strong>Industrial reliability:</strong> Designed for demanding
machine vision applications</li>
</ul>
<hr />
<h3 id="prototype-imu-orange-cube-flight-controller">5.2 Prototype IMU:
Orange Cube Flight Controller</h3>
<p>The Orange Cube flight controller provides high-rate IMU data via
MAVLink protocol, enabling precise motion compensation.</p>
<h4 id="imu-specifications-icm-20948">5.2.1 IMU Specifications
(ICM-20948)</h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Type</td>
<td>9-axis MEMS (3-axis gyro + 3-axis accel + 3-axis mag)</td>
</tr>
<tr>
<td>Gyroscope Range</td>
<td>±250/500/1000/2000 °/s (selectable)</td>
</tr>
<tr>
<td>Gyroscope Resolution</td>
<td>16-bit</td>
</tr>
<tr>
<td>Gyroscope Noise</td>
<td>0.015 °/s/√Hz</td>
</tr>
<tr>
<td>Accelerometer Range</td>
<td>±2/4/8/16 g (selectable)</td>
</tr>
<tr>
<td>Sampling Rate</td>
<td>Up to 1 kHz (gyro), 4.5 kHz (accel)</td>
</tr>
<tr>
<td>Interface</td>
<td>MAVLink over USB/serial</td>
</tr>
</tbody>
</table>
<h4 id="sensor-fusion-1">5.2.2 Sensor Fusion</h4>
<p>The system employs VQF (Versatile Quaternion-based Filter) for sensor
fusion: - Combines gyroscope and accelerometer data - Magnetometer
optional (disabled for moving platform use due to interference) -
Outputs orientation quaternions at IMU sample rate</p>
<hr />
<h3 id="silhil-validation-environment-stellarium">5.3 SIL/HIL Validation
Environment: Stellarium</h3>
<p>The prototype is validated using Stellarium planetarium software to
generate controlled test conditions.</p>
<p><a href="https://www.youtube.com/watch?v=_I9gVQl0hvA"><img
src="https://img.youtube.com/vi/_I9gVQl0hvA/0.jpg"
alt="Stellarium SIL Demonstration" /></a></p>
<p><em>Click thumbnail to watch Stellarium Software-in-the-Loop
demonstration</em></p>
<h4 id="test-setup">5.3.1 Test Setup</h4>
<pre><code>┌─────────────────────────────────────────────────────────────────┐
│                    SIL/HIL TEST ENVIRONMENT                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   ┌─────────────┐        ┌─────────────┐      ┌─────────────┐  │
│   │  Stellarium │        │   Monitor   │      │  Harrier    │  │
│   │ Star Field  │───────▶│  Display    │◀─────│   Camera    │  │
│   │  Generator  │        │             │      │             │  │
│   └─────────────┘        └─────────────┘      └──────┬──────┘  │
│         │                                            │          │
│         │ Configurable:                              │ Video    │
│         │ - Star magnitude                           │ Stream   │
│         │ - Sky rotation                             ▼          │
│         │ - Light pollution        ┌─────────────────────────┐ │
│         │ - Atmospheric effects    │   Processing Pipeline   │ │
│         │                          │   + Orange Cube IMU     │ │
│         │                          └─────────────────────────┘ │
│         │                                            │          │
│         │ Simulated Motion:                          │ Stacked  │
│         │ - Platform vibration                       ▼ Output   │
│         │ - Vehicle dynamics        ┌─────────────────────────┐│
│         │ - Turbulence             │   Validation Metrics    ││
│         └──────────────────────────│   - Star detection rate ││
│                                    │   - Alignment accuracy  ││
│                                    │   - SNR improvement     ││
│                                    └─────────────────────────┘│
└─────────────────────────────────────────────────────────────────┘</code></pre>
<h4 id="validation-capabilities">5.3.2 Validation Capabilities</h4>
<ul>
<li><strong>Controlled star fields:</strong> Known star positions for
accuracy verification</li>
<li><strong>Configurable light pollution:</strong> Test performance in
Bortle 1-9 conditions</li>
<li><strong>Motion injection:</strong> Simulate platform dynamics
(vibration, drift, turbulence)</li>
<li><strong>Repeatability:</strong> Identical test conditions for
algorithm comparison</li>
<li><strong>Ground truth:</strong> Perfect reference for quantitative
performance metrics</li>
</ul>
<hr />
<h3 id="alternative-configuration-a-gopro-hero-7-black">5.4 Alternative
Configuration A: GoPro Hero 7 Black</h3>
<p>The GoPro Hero 7 Black provides an accessible consumer option with
integrated IMU, suitable for portable astrophotography applications.</p>
<h4 id="imaging-specifications-1">5.4.1 Imaging Specifications</h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sensor Type</td>
<td>1/2.3” CMOS</td>
</tr>
<tr>
<td>Resolution</td>
<td>12 MP (4000 × 3000 pixels)</td>
</tr>
<tr>
<td>Pixel Size</td>
<td>1.55 μm</td>
</tr>
<tr>
<td>Lens</td>
<td>Fixed f/2.8, 14-17mm equivalent</td>
</tr>
<tr>
<td>Field of View</td>
<td>Wide: 122.6°, Linear: 94.4°</td>
</tr>
<tr>
<td>Video Modes</td>
<td>4K60, 2.7K120, 1080p240</td>
</tr>
<tr>
<td>Photo Modes</td>
<td>RAW, HDR, Night</td>
</tr>
<tr>
<td>ISO Range</td>
<td>100-6400 (extended: 100-12800)</td>
</tr>
<tr>
<td>Shutter Speed</td>
<td>1/8000s - 30s (photo), 1/fps (video)</td>
</tr>
</tbody>
</table>
<h4 id="embedded-gyroscope">5.4.2 Embedded Gyroscope</h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Type</td>
<td>3-axis MEMS gyroscope</td>
</tr>
<tr>
<td>Sampling Rate</td>
<td>200 Hz (configurable to 400 Hz)</td>
</tr>
<tr>
<td>Range</td>
<td>±2000 °/s</td>
</tr>
<tr>
<td>Resolution</td>
<td>16-bit</td>
</tr>
<tr>
<td>Data Format</td>
<td>GPMF (GoPro Metadata Format)</td>
</tr>
<tr>
<td>Synchronization</td>
<td>Frame-accurate timestamps</td>
</tr>
</tbody>
</table>
<h4 id="astrophotography-settings-protune-mode">5.4.3 Astrophotography
Settings (Protune Mode)</h4>
<p>For optimal night sky capture: - <strong>ISO Min/Max:</strong>
800/6400 - <strong>Shutter:</strong> 1/30s (30fps) or 1/24s (24fps) -
<strong>White Balance:</strong> 5500K (Native) - <strong>Color
Profile:</strong> Flat - <strong>Sharpness:</strong> Low - <strong>Lens
Mode:</strong> Linear (recommended for stacking)</p>
<h4 id="gopro-camera-intrinsic-parameters">5.4.4 GoPro Camera Intrinsic
Parameters</h4>
<p>Calibrated intrinsic matrix for GoPro Hero 7 Black (Linear mode):</p>
<pre><code>K = [fx   0  cx]   [3200    0  2000]
    [ 0  fy  cy] = [   0 3200  1500]
    [ 0   0   1]   [   0    0     1]</code></pre>
<p>Where: - fx, fy = Focal lengths in pixels (~3200) - cx, cy =
Principal point (image center)</p>
<p>Distortion coefficients (Brown-Conrady model): - k1 = -0.25 (radial)
- k2 = 0.08 (radial) - p1, p2 ≈ 0 (tangential, negligible)</p>
<hr />
<h3 id="alternative-configuration-b-zwo-asi585mc-entaniya-m12-220">5.5
Alternative Configuration B: ZWO ASI585MC + Entaniya M12 220</h3>
<p>For applications requiring higher sensitivity, larger field of view,
or all-sky coverage, we provide an enhanced hardware configuration using
dedicated astronomy equipment.</p>
<h4 id="zwo-asi585mc-camera">5.5.1 ZWO ASI585MC Camera</h4>
<p>The ZWO ASI585MC is a high-performance color astronomy camera based
on Sony’s latest STARVIS 2 technology, offering exceptional sensitivity
and low noise characteristics ideal for star tracking and meteor
detection.</p>
<p><strong>Sensor Specifications:</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sensor Model</td>
<td>Sony IMX585 (STARVIS 2)</td>
</tr>
<tr>
<td>Sensor Type</td>
<td>Back-Illuminated CMOS</td>
</tr>
<tr>
<td>Sensor Format</td>
<td>1/1.2” (larger than GoPro’s 1/2.3”)</td>
</tr>
<tr>
<td>Resolution</td>
<td>3840 × 2160 (8.29 MP)</td>
</tr>
<tr>
<td>Pixel Size</td>
<td>2.9 μm × 2.9 μm</td>
</tr>
<tr>
<td>Sensor Dimensions</td>
<td>11.13 mm × 6.26 mm</td>
</tr>
<tr>
<td>Sensor Diagonal</td>
<td>12.84 mm</td>
</tr>
<tr>
<td>Quantum Efficiency (QE)</td>
<td>91% peak (exceptional)</td>
</tr>
<tr>
<td>Full Well Capacity</td>
<td>40,000 e⁻ (3× previous generation)</td>
</tr>
<tr>
<td>Read Noise</td>
<td>0.8 e⁻ (extremely low)</td>
</tr>
<tr>
<td>ADC</td>
<td>12-bit</td>
</tr>
<tr>
<td>Frame Rate</td>
<td>46.9 FPS at full resolution (12-bit)</td>
</tr>
<tr>
<td>Interface</td>
<td>USB 3.0</td>
</tr>
<tr>
<td>Amp Glow</td>
<td>Zero (critical for long exposures)</td>
</tr>
</tbody>
</table>
<p><strong>Key Advantages over GoPro:</strong></p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>ASI585MC</th>
<th>GoPro Hero 7 Black</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sensor Size</td>
<td>1/1.2” (12.84mm diag)</td>
<td>1/2.3” (6.17mm diag)</td>
</tr>
<tr>
<td>Pixel Size</td>
<td>2.9 μm</td>
<td>1.55 μm</td>
</tr>
<tr>
<td>QE Peak</td>
<td>91%</td>
<td>~50% (estimated)</td>
</tr>
<tr>
<td>Read Noise</td>
<td>0.8 e⁻</td>
<td>~3-5 e⁻ (estimated)</td>
</tr>
<tr>
<td>Full Well</td>
<td>40,000 e⁻</td>
<td>~8,000 e⁻ (estimated)</td>
</tr>
<tr>
<td>Amp Glow</td>
<td>Zero</td>
<td>Present in long exposures</td>
</tr>
<tr>
<td>Cooling (Pro)</td>
<td>-35°C below ambient</td>
<td>None</td>
</tr>
<tr>
<td>Raw Output</td>
<td>12-bit uncompressed</td>
<td>10-bit compressed</td>
</tr>
</tbody>
</table>
<p><strong>High Conversion Gain (HCG) Mode:</strong></p>
<p>The ASI585MC features automatic HCG mode activation: - Triggers at
gain ≥252 (standard) or ≥200 (Pro version) - Maintains ~12-bit dynamic
range at high gain - Read noise drops to 0.7-1.5 e⁻ - Optimal for faint
star detection</p>
<p><strong>Spectral Response:</strong></p>
<p>The ASI585MC has enhanced sensitivity in red, green, and
near-infrared (NIR) wavelengths: - Particularly strong in &gt;850nm
range - 1.5× more sensitive than previous IMX485 sensor in NIR -
Excellent for hydrogen-alpha (Hα) nebula detection</p>
<h4 id="entaniya-fisheye-m12-220-lens">5.5.2 Entaniya Fisheye M12 220
Lens</h4>
<p>The Entaniya Fisheye M12 220 is a precision Japanese-manufactured
super wide-angle lens designed for full-hemisphere imaging
applications.</p>
<p><strong>Optical Specifications:</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Field of View</td>
<td>220° (full hemisphere + below horizon)</td>
</tr>
<tr>
<td>Focal Length</td>
<td>1.34 mm</td>
</tr>
<tr>
<td>Aperture</td>
<td>f/2.0 (fixed)</td>
</tr>
<tr>
<td>Optical Formula</td>
<td>10 elements (including 1× ED glass)</td>
</tr>
<tr>
<td>Projection Type</td>
<td>Equidistant (equal angular distortion)</td>
</tr>
<tr>
<td>Image Circle</td>
<td>5.1 mm diameter</td>
</tr>
<tr>
<td>Sensor Compatibility</td>
<td>1/2.3” to 1/1.7” sensors</td>
</tr>
<tr>
<td>Mount Type</td>
<td>M12 × P0.5 (S-mount)</td>
</tr>
<tr>
<td>Weight</td>
<td>52g (without caps)</td>
</tr>
<tr>
<td>AR Coating</td>
<td>Yes (multi-coated)</td>
</tr>
<tr>
<td>Resolution</td>
<td>4K+ edge-to-edge</td>
</tr>
<tr>
<td>Manufacturing</td>
<td>Made in Japan</td>
</tr>
</tbody>
</table>
<p><strong>Included Accessories:</strong> - CS-mount adapter (for
CS-mount cameras) - IR cut filter (removable for full-spectrum) - Allen
wrench and mounting bolts</p>
<p><strong>Equidistant Projection Characteristics:</strong></p>
<p>The equidistant (f-theta) projection maintains linear relationship
between incident angle and image distance:</p>
<pre><code>r = f × θ

Where:
r = radial distance from image center (mm)
f = focal length (1.34 mm)
θ = incident angle from optical axis (radians)</code></pre>
<p>This projection type is ideal for: - Meteor trajectory measurement -
All-sky photometry - Satellite tracking - Aurora monitoring</p>
<p><strong>Distortion Model:</strong></p>
<p>For the Entaniya M12 220 with equidistant projection:</p>
<pre><code>θ_undistorted = r / f
x = r × cos(φ)
y = r × sin(φ)

Where φ = azimuthal angle in image plane</code></pre>
<p>Residual distortion from ideal equidistant: &lt; 1% across full
field</p>
<h4 id="asi585mc-entaniya-system-integration">5.5.3 ASI585MC + Entaniya
System Integration</h4>
<p><strong>Physical Setup:</strong></p>
<pre><code>┌─────────────────────────────────────────────────────────┐
│              ALL-SKY STAR TRACKER CONFIGURATION          │
├─────────────────────────────────────────────────────────┤
│                                                          │
│    ┌─────────────┐                                       │
│    │  Entaniya   │  ← 220° Field of View                 │
│    │  M12 220    │                                       │
│    │   Lens      │                                       │
│    └──────┬──────┘                                       │
│           │ M12 mount                                    │
│    ┌──────┴──────┐                                       │
│    │  ASI585MC   │  ← USB 3.0 to Computer               │
│    │   Camera    │                                       │
│    └──────┬──────┘                                       │
│           │                                              │
│    ┌──────┴──────┐                                       │
│    │   Tripod/   │  ← Pointing at zenith                 │
│    │   Mount     │                                       │
│    └─────────────┘                                       │
│                                                          │
└─────────────────────────────────────────────────────────┘</code></pre>
<p><strong>Camera Intrinsic Parameters (ASI585MC + Entaniya M12
220):</strong></p>
<pre><code>Focal length: f = 1.34 mm
Pixel size: p = 2.9 μm
Focal length in pixels: f_px = f / p = 1340 / 2.9 ≈ 462 pixels

K = [f_px    0    cx ]   [462    0   1920]
    [  0   f_px   cy ] = [  0   462  1080]
    [  0     0     1 ]   [  0    0     1 ]

Image center: (1920, 1080) for 3840×2160 resolution</code></pre>
<p><strong>Angular Resolution:</strong></p>
<pre><code>Pixel scale = arctan(pixel_size / focal_length)
            = arctan(2.9 μm / 1.34 mm)
            = arctan(0.00216)
            ≈ 0.124° = 7.44 arcmin/pixel

For 220° FOV across ~1700 pixel radius:
Effective resolution: ~7.7 arcmin/pixel at image center</code></pre>
<p><strong>Comparison: GoPro vs ASI585MC+Entaniya:</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>GoPro Hero 7 (Linear)</th>
<th>ASI585MC + Entaniya</th>
</tr>
</thead>
<tbody>
<tr>
<td>Field of View</td>
<td>94.4°</td>
<td>220° (full sky)</td>
</tr>
<tr>
<td>Resolution</td>
<td>4000 × 3000</td>
<td>3840 × 2160</td>
</tr>
<tr>
<td>Pixel Scale</td>
<td>~1.8 arcmin/pixel</td>
<td>~7.4 arcmin/pixel</td>
</tr>
<tr>
<td>Light Sensitivity</td>
<td>Moderate</td>
<td>Excellent (91% QE)</td>
</tr>
<tr>
<td>Gyroscope</td>
<td>Built-in 200 Hz</td>
<td>External required</td>
</tr>
<tr>
<td>Use Case</td>
<td>Targeted fields</td>
<td>All-sky monitoring</td>
</tr>
<tr>
<td>Cost</td>
<td>$200-400</td>
<td>$450-600</td>
</tr>
</tbody>
</table>
<h4 id="recommended-settings-for-asi585mc">5.5.4 Recommended Settings
for ASI585MC</h4>
<p><strong>For Star Tracking / All-Sky Imaging:</strong></p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Value</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gain</td>
<td>200-300</td>
<td>HCG mode active, low noise</td>
</tr>
<tr>
<td>Exposure</td>
<td>1-10 seconds</td>
<td>Balance sensitivity vs. trailing</td>
</tr>
<tr>
<td>Binning</td>
<td>1×1</td>
<td>Full resolution</td>
</tr>
<tr>
<td>Format</td>
<td>RAW16</td>
<td>Maximum dynamic range</td>
</tr>
<tr>
<td>USB Traffic</td>
<td>80-100</td>
<td>Balance speed vs. stability</td>
</tr>
<tr>
<td>Cooling (Pro)</td>
<td>-10°C to -20°C</td>
<td>Reduce thermal noise</td>
</tr>
</tbody>
</table>
<p><strong>For Meteor Detection:</strong></p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Value</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gain</td>
<td>300-350</td>
<td>Maximum sensitivity</td>
</tr>
<tr>
<td>Exposure</td>
<td>1/30s (video)</td>
<td>Capture fast-moving objects</td>
</tr>
<tr>
<td>Frame Rate</td>
<td>30 FPS</td>
<td>Adequate temporal resolution</td>
</tr>
<tr>
<td>Format</td>
<td>SER video</td>
<td>Efficient capture format</td>
</tr>
<tr>
<td>Trigger</td>
<td>Motion detection</td>
<td>Automated recording</td>
</tr>
</tbody>
</table>
<h4 id="all-sky-applications">5.5.5 All-Sky Applications</h4>
<p>The ASI585MC + Entaniya M12 220 configuration excels for:</p>
<p><strong>1. Meteor Shower Monitoring</strong> - Full hemisphere
coverage captures all meteor events - High QE detects faint meteors
(magnitude 5-6) - Equidistant projection enables accurate trajectory
reconstruction - High frame rates capture meteor dynamics</p>
<p><strong>2. Satellite Tracking</strong> - Simultaneous tracking of
multiple satellites - LEO to GEO orbit coverage - Starlink constellation
monitoring - Space debris tracking</p>
<p><strong>3. Variable Star Photometry</strong> - All-sky simultaneous
observation - Rapid transient detection - Nova/supernova search -
Exoplanet transit monitoring (bright stars)</p>
<p><strong>4. Weather and Atmospheric Monitoring</strong> - Cloud cover
assessment - Light pollution mapping - Aurora monitoring - Aircraft
tracking</p>
<p><strong>5. Educational Demonstrations</strong> - Real-time
constellation identification - Celestial sphere concepts - Diurnal
motion visualization - Seasonal sky changes</p>
<h3 id="hardware-configuration-comparison">5.6 Hardware Configuration
Comparison</h3>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 11%" />
<col style="width: 9%" />
<col style="width: 24%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr>
<th>Configuration</th>
<th>Cost</th>
<th>FOV</th>
<th>Sensitivity</th>
<th>Best Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Harrier 10x + Orange Cube (Prototype)</strong></td>
<td>$800-1,200</td>
<td>6-57°</td>
<td>Exceptional (0.0004 lux)</td>
<td>Moving platforms, SIL/HIL validation</td>
</tr>
<tr>
<td><strong>GoPro Hero 7 Black</strong></td>
<td>$200-400</td>
<td>94-122°</td>
<td>Moderate</td>
<td>Portable astrophotography, travel</td>
</tr>
<tr>
<td><strong>ASI585MC + Entaniya</strong></td>
<td>$700-900</td>
<td>220°</td>
<td>Excellent</td>
<td>All-sky monitoring, meteor detection</td>
</tr>
<tr>
<td><strong>ASI585MC Pro + Entaniya</strong></td>
<td>$900-1,100</td>
<td>220°</td>
<td>Exceptional</td>
<td>Long-exposure, scientific applications</td>
</tr>
</tbody>
</table>
<h3 id="cost-breakdown-by-configuration">5.7 Cost Breakdown by
Configuration</h3>
<p><strong>Prototype: Harrier 10x + Orange Cube</strong></p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Cost (USD)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Active Silicon Harrier 10x AF-Zoom</td>
<td>$500-700</td>
</tr>
<tr>
<td>Orange Cube Flight Controller</td>
<td>$150-250</td>
</tr>
<tr>
<td>USB 3.0 Capture/Cabling</td>
<td>$50-100</td>
</tr>
<tr>
<td>Processing Computer</td>
<td>$100-200</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>$800-1,250</strong></td>
</tr>
</tbody>
</table>
<p><strong>Alternative A: GoPro Hero 7 Black</strong></p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Cost (USD)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GoPro Hero 7 Black (used)</td>
<td>$150-250</td>
</tr>
<tr>
<td>GoPro Hero 7 Black (new)</td>
<td>$250-400</td>
</tr>
<tr>
<td>MicroSD Card (128GB+)</td>
<td>$20-40</td>
</tr>
<tr>
<td>Processing Computer</td>
<td>$100-200</td>
</tr>
<tr>
<td><strong>Total (used)</strong></td>
<td><strong>$270-490</strong></td>
</tr>
<tr>
<td><strong>Total (new)</strong></td>
<td><strong>$370-640</strong></td>
</tr>
</tbody>
</table>
<p><strong>Alternative B: ASI585MC + Entaniya M12 220</strong></p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Cost (USD)</th>
</tr>
</thead>
<tbody>
<tr>
<td>ZWO ASI585MC Camera</td>
<td>$319-379</td>
</tr>
<tr>
<td>ZWO ASI585MC Pro (cooled)</td>
<td>$499-549</td>
</tr>
<tr>
<td>Entaniya M12 220 Lens</td>
<td>$280-350</td>
</tr>
<tr>
<td>M12 to CS-mount Adapter</td>
<td>Included with lens</td>
</tr>
<tr>
<td>Weather-resistant Housing</td>
<td>$50-100</td>
</tr>
<tr>
<td>USB 3.0 Cable (active, 5m)</td>
<td>$25-40</td>
</tr>
<tr>
<td>Tripod/All-Sky Mount</td>
<td>$50-150</td>
</tr>
<tr>
<td>Processing Computer</td>
<td>Existing or $300+</td>
</tr>
<tr>
<td>Software</td>
<td>Free (open source)</td>
</tr>
<tr>
<td><strong>Total (Standard)</strong></td>
<td><strong>$724-1,019</strong></td>
</tr>
<tr>
<td><strong>Total (Pro/Cooled)</strong></td>
<td><strong>$904-1,189</strong></td>
</tr>
</tbody>
</table>
<hr />
<h2 id="camera-imu-sensor-fusion">6. Camera + IMU Sensor Fusion</h2>
<h3 id="fundamental-concept">6.1 Fundamental Concept</h3>
<p>The core innovation of our star tracker lies in the tight integration
of camera imagery with Inertial Measurement Unit (IMU) orientation data.
This sensor fusion approach enables real-time knowledge of where the
camera is pointing in celestial coordinates, which is essential for:</p>
<ol type="1">
<li><strong>Motion Compensation</strong> - Correcting for camera
movement during exposure</li>
<li><strong>Star Field Prediction</strong> - Knowing which stars should
appear in the field of view</li>
<li><strong>Frame Alignment</strong> - Precise registration of multiple
frames for stacking</li>
<li><strong>Lost-in-Space Solution</strong> - Rapid star identification
using predicted positions</li>
</ol>
<h3 id="physical-setup-rigid-camera-imu-coupling">6.2 Physical Setup:
Rigid Camera-IMU Coupling</h3>
<p>For accurate sensor fusion, the camera and IMU must be rigidly
coupled so that any rotation of the camera is identically measured by
the IMU.</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                    CAMERA + IMU RIGID BODY ASSEMBLY                  │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│         ┌───────────────┐                                           │
│         │   CAMERA      │◄── Optical axis (boresight)               │
│         │   SENSOR      │                                           │
│         │   ┌─────┐     │                                           │
│         │   │     │     │    Field of View                          │
│         │   │ CCD │     │         ╱╲                                │
│         │   │     │     │        ╱  ╲                               │
│         │   └─────┘     │       ╱    ╲                              │
│         └───────┬───────┘      ╱      ╲                             │
│                 │             ╱ Stars  ╲                            │
│         ┌───────┴───────┐                                           │
│         │      IMU      │                                           │
│         │  ┌─────────┐  │                                           │
│         │  │ Gyro    │  │◄── 3-axis angular velocity (ωx, ωy, ωz)  │
│         │  │ Accel   │  │◄── 3-axis acceleration (ax, ay, az)      │
│         │  │ (Mag)   │  │◄── 3-axis magnetometer (optional)        │
│         │  └─────────┘  │                                           │
│         └───────────────┘                                           │
│                                                                      │
│    Rigid coupling ensures: R_camera = R_imu (same orientation)      │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘</code></pre>
<p><strong>Figure 6.0: Prototype Hardware Assembly - Harrier 10x Camera
+ Orange Cube IMU</strong></p>
<table>
<colgroup>
<col style="width: 43%" />
<col style="width: 56%" />
</colgroup>
<thead>
<tr>
<th style="text-align: center;">Front View (Lens)</th>
<th style="text-align: center;">Rear View (Electronics)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><img src="hardware_setup_front.jpeg"
alt="Harrier camera front view with 10x zoom lens mounted on Orange Cube" /></td>
<td style="text-align: center;"><img src="hardware_setup_side.jpeg"
alt="Rear view showing Active Silicon AS-CIB-USBHDMI board and ProfiCNC/HEX Orange Cube" /></td>
</tr>
</tbody>
</table>
<p><em>The prototype assembly uses the Active Silicon Harrier 10x
AF-Zoom camera rigidly mounted on the Orange Cube flight controller via
a 3D-printed bracket (orange). The front features the 10x optical zoom
lens (f=5.1-51mm); the rear shows the AS-CIB-USBHDMI-002-A interface
board (S/N: 71000300) and the ProfiCNC/HEX/uAvionix Orange Cube with
visible I/O connectors (GPS, I2C, USB, CAN, TELEM, ADC).</em></p>
<p><strong>Implementation Options:</strong></p>
<table>
<colgroup>
<col style="width: 32%" />
<col style="width: 23%" />
<col style="width: 21%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr>
<th>Configuration</th>
<th>IMU Source</th>
<th>Coupling</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Harrier 10x + Orange Cube (Prototype)</strong></td>
<td>ICM-20948 9-axis</td>
<td>Rigid bracket</td>
<td>High (calibrated)</td>
</tr>
<tr>
<td>GoPro Hero 7</td>
<td>Built-in BMI260</td>
<td>Factory-calibrated</td>
<td>High</td>
</tr>
<tr>
<td>External IMU + Camera</td>
<td>BNO055, MPU9250</td>
<td>Custom mount</td>
<td>Requires calibration</td>
</tr>
<tr>
<td>ASI585MC + External IMU</td>
<td>Separate 9-DOF</td>
<td>Rigid bracket</td>
<td>Requires calibration</td>
</tr>
</tbody>
</table>
<p><strong>Figure 6.1: Real-Time Stacking Output - Faint Star
Recovery</strong></p>
<table>
<colgroup>
<col style="width: 38%" />
<col style="width: 29%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr>
<th style="text-align: center;">Single Frame (Noisy)</th>
<th style="text-align: center;">Stacked Result</th>
<th style="text-align: center;">Enhanced Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><img
src="images/stacked_20260117_clahe.png" alt="CLAHE Filter" /></td>
<td style="text-align: center;"><img
src="images/stacked_20260117_log.png" alt="Log Filter" /></td>
<td style="text-align: center;"><img
src="images/stacked_20260117_median.png" alt="Median Filter" /></td>
</tr>
</tbody>
</table>
<p><em>The progression demonstrates faint star recovery through
real-time stacking: (Left) Single noisy frame with one visible bright
star; (Center) After stacking multiple frames, a second fainter star
becomes visible; (Right) Enhanced output with reduced noise and improved
star visibility. The green rectangle indicates the template-locked
region of interest used for frame alignment.</em></p>
<h3 id="coordinate-systems-and-transformations">6.3 Coordinate Systems
and Transformations</h3>
<h4 id="reference-frames">6.3.1 Reference Frames</h4>
<p>Four coordinate systems are involved in the sensor fusion:</p>
<pre><code>1. IMU Body Frame (B)
   - Origin: IMU center
   - X: Forward (camera boresight direction)
   - Y: Right
   - Z: Down

2. Camera Frame (C)
   - Origin: Camera optical center
   - X: Right (image +u direction)
   - Y: Down (image +v direction)
   - Z: Forward (optical axis)

3. Local Navigation Frame (N) - North-East-Down (NED)
   - X: North
   - Y: East
   - Z: Down (gravity direction)

4. Celestial Frame (ICRS/J2000)
   - Origin: Solar system barycenter
   - X: Vernal equinox direction
   - Y: 90° east along celestial equator
   - Z: Celestial north pole</code></pre>
<h4 id="transformation-chain">6.3.2 Transformation Chain</h4>
<p>To map a celestial coordinate to image pixels:</p>
<pre><code>                    R_earth        R_local        R_imu         R_cam_imu      K
Celestial (RA,Dec) ────────► NED ────────► Body ────────► Camera ────────► Pixels (u,v)
      ↓                 ↓              ↓              ↓              ↓
   (α, δ)         (LST, lat)    (q_orientation)   (calibrated)   (intrinsics)</code></pre>
<p><strong>Mathematical Formulation:</strong></p>
<pre><code>p_celestial = [cos(δ)cos(α), cos(δ)sin(α), sin(δ)]ᵀ     # Unit vector to star

p_local = R_earth(LST, latitude) · p_celestial           # Transform to local frame

p_body = R_imu · p_local                                 # Apply IMU orientation

p_camera = R_cam_imu · p_body                            # IMU-to-camera alignment

p_image = K · p_camera / p_camera[2]                     # Project to image plane

Where:
- LST = Local Sidereal Time
- R_imu = Rotation matrix from IMU quaternion
- R_cam_imu = Camera-IMU alignment matrix (calibrated)
- K = Camera intrinsic matrix</code></pre>
<h3 id="d-celestial-sphere-visualization">6.4 3D Celestial Sphere
Visualization</h3>
<p>Our system includes a real-time 3D Celestial Sphere Viewer that
visualizes the camera’s orientation relative to the star field. This
tool is invaluable for:</p>
<ul>
<li><strong>Debugging</strong> sensor fusion algorithms</li>
<li><strong>Verifying</strong> IMU-camera alignment</li>
<li><strong>Educational</strong> demonstration of celestial
mechanics</li>
<li><strong>Real-time monitoring</strong> of pointing direction</li>
</ul>
<p><strong>Figure 1: 3D Celestial Sphere Viewer with Camera + IMU
Setup</strong></p>
<figure>
<img src="Screenshot%202026-01-11%20155616.png"
alt="3D Celestial Sphere Viewer showing camera FOV projected onto star field, with live camera feed from rigidly coupled camera and IMU (Orange Cube)" />
<figcaption aria-hidden="true">3D Celestial Sphere Viewer showing camera
FOV projected onto star field, with live camera feed from rigidly
coupled camera and IMU (Orange Cube)</figcaption>
</figure>
<p><em>The screenshot shows the 3D Celestial Sphere Viewer (left)
displaying the camera’s field of view (green trapezoid) projected onto
the celestial sphere with stars. The camera feed (right) shows the
physical setup with the camera and Orange Cube IMU held together,
demonstrating the rigid coupling required for accurate sensor
fusion.</em></p>
<h4 id="visualization-components">6.4.1 Visualization Components</h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                    3D CELESTIAL SPHERE VIEWER                        │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│    ┌─────────────────────────────┐  ┌─────────────────────────────┐ │
│    │                             │  │                             │ │
│    │      CELESTIAL SPHERE       │  │       CAMERA FEED           │ │
│    │                             │  │                             │ │
│    │    ★  ·  ·  ★  ·           │  │    ┌─────────────────────┐  │ │
│    │   ·  ·  ★  ·  ·  ★         │  │    │                     │  │ │
│    │  ·  ┌────────────┐  ·      │  │    │   Live camera       │  │ │
│    │ ★  │  CAMERA    │  ·  ★   │  │    │   preview with      │  │ │
│    │  ·  │    FOV     │  ·      │  │    │   detected stars    │  │ │
│    │   ·  └────────────┘  ·     │  │    │                     │  │ │
│    │    ★  ·  ·  ★  ·  ★        │  │    └─────────────────────┘  │ │
│    │                             │  │                             │ │
│    │  ── Celestial equator       │  │   IMU: [w,x,y,z]           │ │
│    │  ── Ecliptic                │  │   RA: 12h 34m 56s          │ │
│    │  ── Local horizon           │  │   Dec: +45° 12&#39; 34&quot;        │ │
│    │                             │  │                             │ │
│    └─────────────────────────────┘  └─────────────────────────────┘ │
│                                                                      │
│    Controls:                                                         │
│    ─────────────────────────────────────────────────────────────    │
│    Left Mouse Drag - Rotate view    R - Reset view                  │
│    Scroll Wheel    - Zoom in/out    G - Toggle grid                 │
│    C - Toggle constellations        F - Toggle FOV display          │
│    Space - Pause/Resume             Q/ESC - Quit                    │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘</code></pre>
<h4 id="camera-field-of-view-projection">6.4.2 Camera Field of View
Projection</h4>
<p>The camera’s field of view is projected onto the celestial sphere as
a quadrilateral (frustum intersection):</p>
<pre><code>Given camera orientation quaternion q and intrinsic matrix K:

1. Define image corners: (0,0), (W,0), (W,H), (0,H)

2. Back-project to unit rays:
   ray_i = K⁻¹ · [u_i, v_i, 1]ᵀ
   ray_i = ray_i / ||ray_i||

3. Rotate to celestial frame:
   celestial_ray_i = R(q)ᵀ · R_earth(t)ᵀ · ray_i

4. Convert to spherical coordinates:
   RA_i = atan2(y_i, x_i)
   Dec_i = asin(z_i)

5. Draw FOV polygon on celestial sphere connecting the 4 corners</code></pre>
<h3 id="imu-camera-calibration">6.5 IMU-Camera Calibration</h3>
<p>For systems with external IMUs, the relative orientation between IMU
and camera must be calibrated.</p>
<h4 id="calibration-procedure">6.5.1 Calibration Procedure</h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                    IMU-CAMERA CALIBRATION PROCEDURE                  │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  Step 1: Mount camera and IMU rigidly together                      │
│          ┌─────────┐                                                │
│          │ Camera  │                                                │
│          └────┬────┘                                                │
│          ┌────┴────┐                                                │
│          │   IMU   │  ◄── Approximate alignment                     │
│          └─────────┘                                                │
│                                                                      │
│  Step 2: Capture calibration sequence                               │
│          - Point at known star field (use planetarium software)     │
│          - Rotate assembly through multiple orientations            │
│          - Record IMU data + star positions in each frame           │
│                                                                      │
│  Step 3: Solve for R_cam_imu                                        │
│          - Detect stars in images → measured positions              │
│          - Query star catalog → true celestial positions            │
│          - IMU provides R_imu for each frame                        │
│          - Solve: R_cam_imu = argmin Σ ||p_measured - p_predicted||²│
│                                                                      │
│  Step 4: Validate calibration                                       │
│          - Point at different star field                            │
│          - Verify predicted vs measured star positions              │
│          - Reprojection error should be &lt; 1 pixel                   │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘</code></pre>
<h4 id="calibration-matrix">6.5.2 Calibration Matrix</h4>
<p>The IMU-to-camera rotation is typically a small correction:</p>
<pre><code>R_cam_imu = R_z(ψ) · R_y(θ) · R_x(φ)

Where:
- φ (roll): Rotation about optical axis
- θ (pitch): Rotation about camera Y axis
- ψ (yaw): Rotation about camera Z axis

For Harrier + Orange Cube (Prototype): R_cam_imu calibrated via rigid bracket alignment
For GoPro with built-in IMU: R_cam_imu ≈ I (identity, factory-calibrated)
For external IMU setups: Typically |φ|, |θ|, |ψ| &lt; 5° after careful mounting</code></pre>
<h3 id="sensor-fusion-algorithms">6.6 Sensor Fusion Algorithms</h3>
<h4 id="gyroscope-only-integration">6.6.1 Gyroscope-Only
Integration</h4>
<p>The simplest approach integrates gyroscope angular velocities:</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> integrate_gyro(gyro_data, dt):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Dead-reckoning orientation from gyroscope.&quot;&quot;&quot;</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> Quaternion(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)  <span class="co"># Identity (initial orientation)</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> omega <span class="kw">in</span> gyro_data:</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Quaternion derivative: dq/dt = 0.5 * q ⊗ [0, ω]</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        omega_quat <span class="op">=</span> Quaternion(<span class="dv">0</span>, omega[<span class="dv">0</span>], omega[<span class="dv">1</span>], omega[<span class="dv">2</span>])</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        q_dot <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> q <span class="op">*</span> omega_quat</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Integrate</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q <span class="op">+</span> q_dot <span class="op">*</span> dt</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q.normalized()</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q</span></code></pre></div>
<p><strong>Limitations:</strong> - Gyroscope bias causes drift
(~1-10°/hour) - No absolute reference (only relative orientation) -
Accumulating errors over time</p>
<h4 id="vqf-sensor-fusion-recommended">6.6.2 VQF Sensor Fusion
(Recommended)</h4>
<p>The Versatile Quaternion-based Filter (VQF) combines gyroscope and
accelerometer for drift-free orientation:</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                         VQF SENSOR FUSION                            │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│    Gyroscope ω ──┐                                                  │
│                  │     ┌──────────────┐                             │
│                  ├────►│   Gyroscope  │                             │
│                  │     │  Integration │────┐                        │
│    Bias b̂ ──────┘     └──────────────┘    │                        │
│                                            │    ┌────────────────┐  │
│                              ┌─────────────┼───►│   Quaternion   │  │
│                              │             │    │    Output      │──►│
│    Accelerometer a ──┐       │             │    │   q(t)         │  │
│                      │  ┌────┴─────┐       │    └────────────────┘  │
│                      └─►│   Tilt   │───────┘                        │
│                         │Correction│                                │
│                         └──────────┘                                │
│                              ▲                                      │
│                              │                                      │
│                         ┌────┴─────┐                                │
│                         │   Rest   │                                │
│                         │Detection │                                │
│                         └──────────┘                                │
│                                                                      │
│    Key Features:                                                    │
│    • Online gyroscope bias estimation                               │
│    • Accelerometer-based tilt correction (gravity reference)        │
│    • Rest detection for improved bias estimation                    │
│    • Adaptive gain based on motion dynamics                         │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘</code></pre>
<p><strong>VQF Algorithm Steps:</strong></p>
<ol type="1">
<li><p><strong>Gyroscope Integration:</strong></p>
<pre><code>q_gyro(t) = q(t-1) ⊗ exp([0, (ω - b̂) · dt / 2])</code></pre></li>
<li><p><strong>Tilt Correction (from accelerometer):</strong></p>
<pre><code>g_measured = R(q_gyro) · [0, 0, 1]ᵀ  # Expected gravity in body frame
g_actual = normalize(accelerometer)   # Measured gravity

# Correction quaternion
q_correction = quaternion_from_vectors(g_measured, g_actual)
q(t) = slerp(q_gyro, q_gyro ⊗ q_correction, α)</code></pre></li>
<li><p><strong>Bias Estimation (during rest):</strong></p>
<pre><code>if is_stationary(accelerometer):
    b̂ = b̂ + β · (ω - b̂)  # Update bias estimate</code></pre></li>
</ol>
<h4 id="star-aided-correction">6.6.3 Star-Aided Correction</h4>
<p>For highest accuracy, detected stars provide absolute orientation
correction:</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                    STAR-AIDED ORIENTATION CORRECTION                 │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│    IMU Orientation ───────┐                                         │
│    q_imu(t)               │                                         │
│                           ▼                                         │
│                    ┌──────────────┐                                 │
│                    │   Predict    │                                 │
│    Star Catalog ──►│    Star      │──► Expected star positions      │
│                    │  Positions   │    in image                     │
│                    └──────────────┘                                 │
│                           │                                         │
│                           ▼                                         │
│                    ┌──────────────┐                                 │
│    Camera Frame ──►│    Match     │                                 │
│    (detected      │    Stars     │                                 │
│     stars)        └──────────────┘                                 │
│                           │                                         │
│                           ▼                                         │
│                    ┌──────────────┐                                 │
│                    │   Compute    │                                 │
│                    │  Correction  │──► Δq (orientation error)       │
│                    └──────────────┘                                 │
│                           │                                         │
│                           ▼                                         │
│                    ┌──────────────┐                                 │
│                    │    Apply     │                                 │
│                    │   Kalman     │──► q_corrected(t)               │
│                    │   Update     │                                 │
│                    └──────────────┘                                 │
│                                                                      │
│    Benefits:                                                        │
│    • Eliminates gyroscope drift completely                          │
│    • Provides absolute celestial orientation                        │
│    • Sub-arcminute accuracy achievable                              │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘</code></pre>
<h3 id="benefits-for-star-tracking">6.7 Benefits for Star Tracking</h3>
<p>The camera+IMU sensor fusion provides critical advantages:</p>
<table>
<colgroup>
<col style="width: 28%" />
<col style="width: 30%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr>
<th>Capability</th>
<th>Without IMU</th>
<th>With IMU Fusion</th>
</tr>
</thead>
<tbody>
<tr>
<td>Motion compensation</td>
<td>Optical flow (slow)</td>
<td>Direct quaternion transform (fast)</td>
</tr>
<tr>
<td>Frame alignment</td>
<td>Feature matching</td>
<td>Predicted + refined</td>
</tr>
<tr>
<td>Star identification</td>
<td>Lost-in-space search</td>
<td>Catalog lookup by position</td>
</tr>
<tr>
<td>Exposure during motion</td>
<td>Blurred stars</td>
<td>Sharp (compensated)</td>
</tr>
<tr>
<td>Real-time preview</td>
<td>Not possible</td>
<td>Live celestial overlay</td>
</tr>
<tr>
<td>Processing speed</td>
<td>1-5 fps</td>
<td>30+ fps</td>
</tr>
</tbody>
</table>
<h3 id="video-demonstration">6.8 Video Demonstration</h3>
<p>A demonstration of the camera+IMU fusion system is available:</p>
<p><a href="https://youtube.com/shorts/96TbY9RKdZE"><img
src="https://img.youtube.com/vi/96TbY9RKdZE/0.jpg"
alt="Camera+IMU Fusion Demo" /></a></p>
<p><em>Click thumbnail to watch Camera + IMU Sensor Fusion
demonstration</em></p>
<p>The video shows: 1. Physical setup with camera and IMU (orange cube)
rigidly coupled 2. Real-time 3D Celestial Sphere Viewer tracking camera
orientation 3. Camera field of view projected onto the star field 4.
Responsive orientation updates as the assembly is rotated</p>
<h3 id="hybrid-stabilization-gyroscope-template-matching">6.9 Hybrid
Stabilization: Gyroscope + Template Matching</h3>
<p>A significant advancement in our stabilization approach combines
gyroscope-based compensation with template matching refinement,
achieving superior performance compared to either method alone.</p>
<p><a href="https://youtu.be/0RTEzAB1f-k?t=118"><img
src="https://img.youtube.com/vi/0RTEzAB1f-k/0.jpg"
alt="Hybrid Stabilization Demo" /></a></p>
<p><em>Click thumbnail to watch hybrid real-time video stabilization and
ROI stacking demonstration</em></p>
<h4 id="the-problem-with-single-method-approaches">6.9.1 The Problem
with Single-Method Approaches</h4>
<p><strong>Pure Template Matching Limitations:</strong> - <strong>Roll
sensitivity</strong>: Template matching is highly susceptible to
rotation. Even small roll angles cause the template to fail matching
because the pattern rotates. - <strong>Large search windows
needed</strong>: Without prior motion estimation, template matching must
search a large area, which is computationally expensive, prone to false
matches, and unable to handle fast motion.</p>
<p><strong>Pure Gyro Stabilization Limitations:</strong> - <strong>Drift
over time</strong>: IMU integration accumulates errors - <strong>No
absolute reference</strong>: Cannot correct if the initial frame was not
level - <strong>Sensor noise</strong>: High-frequency jitter passes
through - <strong>Mounting offsets</strong>: Calibration between camera
and IMU axes can introduce errors</p>
<h4 id="the-hybrid-solution">6.9.2 The Hybrid Solution</h4>
<p>By applying IMU gyro compensation FIRST, then applying template
matching for refinement:</p>
<pre><code>Raw Frame
    │
    ▼
┌─────────────────────────────────────────────────────────────┐
│  STEP 1: Gyro-Based Roll/Pitch Compensation                 │
│                                                             │
│  • Roll correction: Rotate frame to level horizon           │
│  • Pitch correction: Vertical shift based on VFOV           │
│                                                             │
│  IMU STRENGTH: Fast response, handles rotation well         │
│                Low latency (100 Hz attitude data)           │
└─────────────────────────────────────────────────────────────┘
    │
    ▼ (Pre-leveled frame - rotation removed!)
    │
┌─────────────────────────────────────────────────────────────┐
│  STEP 2: Template Matching XY Refinement                    │
│                                                             │
│  • Now receives LEVELED image (no rotation!)                │
│  • Can use SMALL search window (gyro already compensated)   │
│  • Only needs to find XY translation residual               │
│                                                             │
│  TEMPLATE STRENGTH: Sub-pixel precision, drift correction   │
│                     Absolute reference to scene content     │
└─────────────────────────────────────────────────────────────┘
    │
    ▼ (Fully stabilized frame)</code></pre>
<h4 id="why-sensor-fusion-works-well">6.9.3 Why Sensor Fusion Works
Well</h4>
<ol type="1">
<li><p><strong>Roll Removal Enables Template Matching</strong></p>
<ul>
<li>Template matching is extremely sensitive to rotation</li>
<li>By removing roll first via gyro, the template sees a consistently
oriented image</li>
<li>Match quality (correlation) stays high (&gt;0.7) even during
motion</li>
</ul></li>
<li><p><strong>Pitch Compensation Reduces Search Area</strong></p>
<ul>
<li>Without pitch compensation, vertical motion requires large search
margins</li>
<li>Gyro-based pitch shift pre-positions the frame</li>
<li>Template matching only needs to find small residual offsets</li>
<li>Smaller search window = faster matching + fewer false positives</li>
</ul></li>
<li><p><strong>Complementary Strengths</strong></p>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 27%" />
<col style="width: 51%" />
</colgroup>
<thead>
<tr>
<th>Aspect</th>
<th>IMU Gyro</th>
<th>Template Matching</th>
</tr>
</thead>
<tbody>
<tr>
<td>Speed</td>
<td>Fast (100 Hz)</td>
<td>Slower (per-frame)</td>
</tr>
<tr>
<td>Rotation handling</td>
<td>Excellent</td>
<td>Poor</td>
</tr>
<tr>
<td>Translation accuracy</td>
<td>Good (with VFOV scaling)</td>
<td>Excellent (sub-pixel)</td>
</tr>
<tr>
<td>Drift</td>
<td>Accumulates</td>
<td>None (absolute)</td>
</tr>
<tr>
<td>Latency</td>
<td>Very low</td>
<td>Moderate</td>
</tr>
</tbody>
</table></li>
<li><p><strong>Reduced Uncertainty</strong></p>
<ul>
<li>Gyro minimizes the uncertainty space for template matching</li>
<li>Instead of searching full frame, search ±200px around expected
position</li>
<li>Higher confidence matches, fewer outliers</li>
</ul></li>
</ol>
<h4 id="template-matching-success-rate">6.9.4 Template Matching Success
Rate</h4>
<table>
<thead>
<tr>
<th>Configuration</th>
<th>Match Quality</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Template only (no gyro)</td>
<td>0.3-0.5</td>
<td>Fails during roll</td>
</tr>
<tr>
<td>Gyro only (no template)</td>
<td>N/A</td>
<td>Drifts over time</td>
</tr>
<tr>
<td>Gyro + Template Hybrid</td>
<td>0.7-0.95</td>
<td>Stable even during motion</td>
</tr>
</tbody>
</table>
<h4 id="implementation-dual-gui-system">6.9.5 Implementation: Dual GUI
System</h4>
<p>The hybrid system is implemented as a dual-window application,
demonstrated using: - <strong>Camera</strong>: Harrier 10x AF Zoom
(1280×720 @ 60 FPS) - <strong>IMU</strong>: Orange Cube flight
controller (MAVLink ATTITUDE @ 100 Hz)</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                        DUAL GUI SYSTEM                               │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  ┌─────────────────────────────────────────────────────────────┐    │
│  │              STABILIZER (Real-time OpenCV Window)            │    │
│  │                                                              │    │
│  │   Camera → Gyro Roll/Pitch → Template XY → Stabilized Frame │    │
│  │      ↓           ↓                ↓              ↓          │    │
│  │   60 FPS    Orange Cube      Fine-tune      Sent to Queue   │    │
│  └──────────────────────────────────────────────────────────────┘    │
│                              │                                       │
│                              ▼ (Frame Queue)                         │
│                                                                      │
│  ┌─────────────────────────────────────────────────────────────┐    │
│  │              STACKER (Tkinter GUI + OpenCV Window)           │    │
│  │                                                              │    │
│  │   Receive Frame → Crop Template Region → Stack → Enhance    │    │
│  │        ↓                   ↓               ↓         ↓      │    │
│  │   Template Info       Aligned Crop      max/avg   CLAHE     │    │
│  │   from Stabilizer                       median    asinh     │    │
│  └──────────────────────────────────────────────────────────────┘    │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘</code></pre>
<p><strong>Stabilizer Window Features:</strong> - Real-time display at
camera FPS (60 Hz) - Draw rectangle to set template region - Green box
when match quality &gt; 0.7, yellow otherwise - Keyboard controls: B =
reset baseline, R = clear template, Q = quit</p>
<p><strong>Stacker GUI Features:</strong> - Receives stabilized frames
via thread-safe queue - Automatically uses template region from
stabilizer - Adjustable stack count (2-100 frames), intensity multiplier
(1-10×) - Multiple stack modes: max, average, sum, median - Adaptive
enhancement options: CLAHE, asinh, log, sqrt stretches</p>
<h3 id="simple-star-solver-proof-of-concept">6.10 Simple Star Solver
(Proof of Concept)</h3>
<p>The <code>live_simple_star_solve.py</code> script demonstrates
real-time star pattern matching and celestial coordinate estimation
using a simplified plate-solving approach.</p>
<p><strong>Figure 6.2: Simple Star Solver - Live Plate Solving
POC</strong></p>
<figure>
<img src="homemade_starsolver.png"
alt="Simple Star Solver showing real-time star detection and celestial coordinate estimation" />
<figcaption aria-hidden="true">Simple Star Solver showing real-time star
detection and celestial coordinate estimation</figcaption>
</figure>
<p><em>The screenshot shows the Simple Star Solver tracking 50 detected
stars with 100% confidence. The estimated RA/Dec coordinates are
compared against Stellarium reference values, achieving ~5 arcminute
accuracy at 60° FOV. Real-time performance: 18-22 FPS with 21.4 FPS
processing rate.</em></p>
<h4 id="features">6.10.1 Features</h4>
<ul>
<li><strong>Real-time star detection:</strong> Detects up to 50+ stars
per frame at 20+ FPS</li>
<li><strong>Coordinate estimation:</strong> Estimates RA/Dec from
detected star patterns</li>
<li><strong>Stellarium validation:</strong> Compares against Stellarium
reference for accuracy verification</li>
<li><strong>Confidence scoring:</strong> Reports match quality
(0-100%)</li>
<li><strong>Error metrics:</strong> Shows angular error in
arcminutes/degrees</li>
</ul>
<h4 id="performance-metrics">6.10.2 Performance Metrics</h4>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stars Detected</td>
<td>50</td>
</tr>
<tr>
<td>Confidence</td>
<td>74-100%</td>
</tr>
<tr>
<td>Angular Error</td>
<td>~5 arcmin (RA), ~14 arcmin (Dec)</td>
</tr>
<tr>
<td>Processing Rate</td>
<td>18-22 FPS</td>
</tr>
<tr>
<td>FOV</td>
<td>60°</td>
</tr>
</tbody>
</table>
<h4 id="usage">6.10.3 Usage</h4>
<div class="sourceCode" id="cb27"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> live_simple_star_solve.py</span></code></pre></div>
<p>The solver uses triangle pattern matching against a star catalog to
identify the star field and compute the camera’s pointing direction in
celestial coordinates.</p>
<hr />
<h2 id="imu-based-motion-deblur">7. IMU-Based Motion Deblur</h2>
<p>During long-exposure astrophotography, camera motion causes stars to
appear as trails (motion blur) instead of points. Traditional solutions
require expensive motorized tracking mounts. Our approach uses IMU
attitude data to computationally reverse the motion blur, recovering
sharp star images from blurred exposures.</p>
<h3 id="the-motion-blur-problem">7.1 The Motion Blur Problem</h3>
<p>Motion blur in star field images occurs when the camera orientation
changes during exposure. The resulting image is a convolution of the
sharp star field with a Point Spread Function (PSF) that encodes the
motion trajectory:</p>
<pre><code>I_blurred(x,y) = I_sharp(x,y) * PSF(x,y) + noise</code></pre>
<p>Where <code>*</code> denotes convolution and PSF represents the
motion blur kernel.</p>
<p><strong>Figure 7.1: Motion Deblur Pipeline</strong></p>
<figure>
<img src="images/motion_deblur/deblur_pipeline.png"
alt="Motion Deblur Pipeline" />
<figcaption aria-hidden="true">Motion Deblur Pipeline</figcaption>
</figure>
<h3 id="psf-generation-from-imu-data">8.2 PSF Generation from IMU
Data</h3>
<p>The key insight is that the IMU provides a complete record of camera
orientation during exposure. By tracking how each pixel moves across the
image plane, we can reconstruct the exact PSF.</p>
<h4 id="quaternion-trajectory-to-pixel-motion">8.2.1 Quaternion
Trajectory to Pixel Motion</h4>
<p>For a point at image coordinates (x, y):</p>
<ol type="1">
<li><p><strong>Back-project</strong> to 3D direction using camera
intrinsics:</p>
<pre><code>d_camera = K⁻¹ · [x, y, 1]ᵀ</code></pre></li>
<li><p><strong>Transform</strong> to world frame using reference
orientation:</p>
<pre><code>d_world = R_ref · d_camera</code></pre></li>
<li><p><strong>Project</strong> through each IMU sample to get
trajectory:</p>
<pre><code>For each quaternion q(t):
    d_current = R(q(t)) · d_world
    (x(t), y(t)) = project(d_current)</code></pre></li>
</ol>
<p><strong>Figure 7.2: PSF Visualization for Different Motion
Types</strong></p>
<figure>
<img src="images/motion_deblur/psf_visualization.png"
alt="PSF Visualization" />
<figcaption aria-hidden="true">PSF Visualization</figcaption>
</figure>
<p>The figure shows PSFs generated from three motion scenarios: -
<strong>Drift Only</strong>: Linear blur from constant angular rate -
<strong>Vibration Only</strong>: Complex pattern from periodic motion -
<strong>Combined</strong>: Realistic blur combining drift and
vibration</p>
<h3 id="deconvolution-algorithms">8.3 Deconvolution Algorithms</h3>
<p>Given the PSF, we recover the sharp image using deconvolution.</p>
<h4 id="richardson-lucy-algorithm">8.3.1 Richardson-Lucy Algorithm</h4>
<p>An iterative maximum-likelihood method well-suited for Poisson noise
(star photon counting):</p>
<pre><code>estimate(n+1) = estimate(n) · [image / (estimate(n) * PSF)] * PSF_flipped</code></pre>
<p><strong>Advantages:</strong> - Preserves positivity (important for
star images) - Handles Poisson noise characteristics - Converges to
maximum likelihood solution</p>
<p><strong>Parameters:</strong> - Iterations: 20-30 (balance between
sharpness and noise amplification) - PSF kernel size: Depends on motion
extent (typically 51-101 pixels)</p>
<h4 id="wiener-filter">8.3.2 Wiener Filter</h4>
<p>A frequency-domain approach with explicit noise regularization:</p>
<pre><code>H_wiener = H* / (|H|² + K)</code></pre>
<p>Where K is the noise-to-signal ratio estimate.</p>
<p><strong>Advantages:</strong> - Single-pass computation (faster) -
Explicit noise suppression</p>
<h3 id="the-star-trail-overlap-problem">8.4 The Star Trail Overlap
Problem</h3>
<p>A critical corner case occurs when the frame shift is large
(significant angular motion during long exposures). Star trails from
opposite sides of the image can overlap.</p>
<p><strong>Figure 7.3: Star Trail Overlap Concept</strong></p>
<figure>
<img src="images/motion_deblur/overlap_concept_diagram.png"
alt="Overlap Concept" />
<figcaption aria-hidden="true">Overlap Concept</figcaption>
</figure>
<p><strong>The Problem:</strong> - Frame shifts right during exposure -
Star A (near left edge) produces a trail extending rightward before
exiting the frame - Star B enters from the right side, its trail also
extends rightward - The END of Star A’s trail overlaps with the START of
Star B’s trail - Standard deconvolution cannot separate these
overlapping signals</p>
<h4 id="overlap-detection">8.4.1 Overlap Detection</h4>
<p>We detect overlap conditions by analyzing the total frame shift:</p>
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>frame_shift <span class="op">=</span> compute_total_motion(imu_data)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">abs</span>(frame_shift.x) <span class="op">&gt;</span> <span class="fl">0.3</span> <span class="op">*</span> image_width:</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    overlap_detected <span class="op">=</span> <span class="va">True</span></span></code></pre></div>
<h4 id="time-windowed-deconvolution">8.4.2 Time-Windowed
Deconvolution</h4>
<p>Our solution segments the exposure into time windows and processes
overlap regions separately:</p>
<ol type="1">
<li><strong>Segment</strong> the image into regions based on frame shift
direction</li>
<li><strong>Generate time-windowed PSFs</strong> for each region:
<ul>
<li>Edge regions use partial-exposure PSFs</li>
<li>Center regions use full-exposure PSF</li>
</ul></li>
<li><strong>Deconvolve</strong> each region independently</li>
<li><strong>Blend</strong> results with smooth transitions</li>
</ol>
<p><strong>Figure 7.4: Overlap Handling Comparison</strong></p>
<figure>
<img src="images/motion_deblur/overlap_handling_comparison.png"
alt="Overlap Handling Comparison" />
<figcaption aria-hidden="true">Overlap Handling Comparison</figcaption>
</figure>
<p>The comparison shows: - Top-left: Sharp reference image - Top-right:
Motion-blurred image with large drift - Bottom-left: Deblurred WITHOUT
overlap handling (artifacts at edges) - Bottom-right: Deblurred WITH
overlap handling (improved edge recovery)</p>
<h3 id="results-motion-deblur-performance">8.5 Results: Motion Deblur
Performance</h3>
<p><strong>Figure 7.5: Basic Motion Deblur Results</strong></p>
<figure>
<img src="images/motion_deblur/basic_deblur_comparison.png"
alt="Basic Deblur Comparison" />
<figcaption aria-hidden="true">Basic Deblur Comparison</figcaption>
</figure>
<p>Typical performance metrics: - PSNR improvement: 3-8 dB (depending on
motion severity) - Star recovery rate: &gt;90% of blurred stars become
detectable as points - Processing time: &lt;5 seconds for 1920x1080
image (25 iterations)</p>
<h3 id="implementation">8.6 Implementation</h3>
<p>The motion deblur module is implemented in Python and available in
<code>motion_deblur/</code>:</p>
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> motion_deblur <span class="im">import</span> MotionDeblur, IMUData, DeblurParams</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load IMU data from Orange Cube</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>imu_data <span class="op">=</span> IMUData(timestamps, quaternions)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create deblur processor</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>deblur <span class="op">=</span> MotionDeblur(</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    image_width<span class="op">=</span><span class="dv">1920</span>,</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    image_height<span class="op">=</span><span class="dv">1080</span>,</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    focal_length_px<span class="op">=</span><span class="dv">1200</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Deblur with overlap handling</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> DeblurParams(</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    method<span class="op">=</span><span class="st">&#39;richardson_lucy&#39;</span>,</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    iterations<span class="op">=</span><span class="dv">25</span>,</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>    handle_overlap<span class="op">=</span><span class="va">True</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>result, metadata <span class="op">=</span> deblur.deblur(blurred_image, imu_data, params)</span></code></pre></div>
<h3 id="practical-considerations">7.7 Practical Considerations</h3>
<p><strong>When to use motion deblur:</strong> - Long exposures (&gt;5
seconds) without tracking mount - Handheld or vehicle-mounted operation
- Recovering images from unstable platforms</p>
<p><strong>Limitations:</strong> - Requires synchronized IMU data during
exposure - Very large motion (&gt;45°) degrades recovery quality - Noise
amplification increases with blur severity</p>
<p><strong>Best practices:</strong> - Keep exposure short enough that
blur &lt; 50 pixels - Use vibration isolation when possible - Combine
with frame stacking for optimal results</p>
<hr />
<h2 id="software-algorithms">8. Software Algorithms</h2>
<h3 id="gyroscope-data-processing">8.1 Gyroscope Data Processing</h3>
<h4 id="gpmf-extraction">8.1.1 GPMF Extraction</h4>
<p>The GoPro Metadata Format (GPMF) embeds telemetry data within MP4
files. Our extractor:</p>
<ol type="1">
<li>Parses MP4 container for metadata tracks</li>
<li>Extracts GYRO streams (angular velocity)</li>
<li>Extracts ACCL streams (acceleration, optional)</li>
<li>Synchronizes timestamps with video frames</li>
</ol>
<h4 id="bias-estimation">8.1.2 Bias Estimation</h4>
<p>Gyroscope bias is estimated from stationary periods:</p>
<pre><code>ω_bias = (1/N) Σ ω(t)  for t ∈ [t_start, t_start + Δt] ∪ [t_end - Δt, t_end]</code></pre>
<p>Where Δt is typically 1-2 seconds of assumed stationary
recording.</p>
<h4 id="orientation-integration">8.1.3 Orientation Integration</h4>
<p>Angular velocities are integrated to obtain orientation quaternions
using 4th-order Runge-Kutta (RK4):</p>
<p><strong>Quaternion derivative:</strong></p>
<pre><code>dq/dt = (1/2) q ⊗ [0, ω_x, ω_y, ω_z]</code></pre>
<p><strong>RK4 Integration:</strong></p>
<pre><code>k1 = h · f(t, q)
k2 = h · f(t + h/2, q + k1/2)
k3 = h · f(t + h/2, q + k2/2)
k4 = h · f(t + h, q + k3)
q(t+h) = normalize(q + (k1 + 2k2 + 2k3 + k4)/6)</code></pre>
<h4 id="filtering">8.1.4 Filtering</h4>
<p>A low-pass Butterworth filter removes high-frequency noise:</p>
<ul>
<li><strong>Cutoff frequency:</strong> 50 Hz</li>
<li><strong>Order:</strong> 4th order</li>
<li><strong>Phase:</strong> Zero-phase (forward-backward filtering)</li>
</ul>
<h4 id="sensor-fusion-architecture">8.1.5 Sensor Fusion
Architecture</h4>
<p>The system implements a <strong>complementary filter</strong>
approach for fusing gyroscope and accelerometer data, chosen for
computational efficiency and suitability for short observation
periods.</p>
<p><strong>Filter Design:</strong></p>
<pre><code>q_fused = α · q_gyro + (1-α) · q_accel_correction</code></pre>
<p>Where: - <code>α = τ / (τ + dt)</code> is the filter coefficient -
<code>τ = 2.0 seconds</code> (time constant, tunable) -
<code>dt = 1/200 = 0.005 seconds</code> (sample period at 200 Hz)</p>
<p><strong>Accelerometer Correction (Tilt-Only):</strong></p>
<p>The accelerometer provides gravity vector direction for roll/pitch
correction:</p>
<pre><code>g_measured = [ax, ay, az] / |[ax, ay, az]|
g_reference = [0, 0, -1]  # Gravity in world frame

roll  = atan2(ay, az)
pitch = atan2(-ax, sqrt(ay² + az²))</code></pre>
<p><strong>Magnetometer Usage:</strong></p>
<p>For both the prototype (Orange Cube) and GoPro configurations, the
magnetometer is <strong>not used</strong> due to: 1. Electromagnetic
interference on moving platforms (vehicles, aircraft, marine vessels) 2.
Local magnetic disturbances in typical environments 3. Sufficient
accuracy from gyro-only integration for short observations 4.
Calibration complexity for consumer hardware</p>
<p>For extended observations (&gt;5 minutes), star-aided drift
correction is recommended over magnetometer fusion.</p>
<p><strong>Quaternion Normalization:</strong></p>
<p>Quaternions are normalized after every integration step:</p>
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normalize_quaternion(q):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    norm <span class="op">=</span> np.sqrt(q[<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> q[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> q[<span class="dv">2</span>]<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> q[<span class="dv">3</span>]<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> norm <span class="op">&lt;</span> <span class="fl">1e-10</span>:</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array([<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>])</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q <span class="op">/</span> norm</span></code></pre></div>
<p>Normalization threshold: <code>||q|| - 1.0 &gt; 1e-6</code> triggers
renormalization.</p>
<p><strong>Noise Parameters:</strong></p>
<p><em>Orange Cube (ICM-20948) - Prototype:</em></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
<th>Units</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gyroscope noise density</td>
<td>0.015</td>
<td>°/s/√Hz</td>
</tr>
<tr>
<td>Gyroscope bias stability</td>
<td>5.0</td>
<td>°/hour</td>
</tr>
<tr>
<td>Accelerometer noise density</td>
<td>230</td>
<td>µg/√Hz</td>
</tr>
<tr>
<td>Sample rate</td>
<td>Up to 1000</td>
<td>Hz</td>
</tr>
</tbody>
</table>
<p><em>GoPro Hero 7 Black (Alternative):</em></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
<th>Units</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gyroscope noise density</td>
<td>0.007</td>
<td>°/s/√Hz</td>
</tr>
<tr>
<td>Gyroscope bias stability</td>
<td>3.0</td>
<td>°/hour</td>
</tr>
<tr>
<td>Accelerometer noise density</td>
<td>150</td>
<td>µg/√Hz</td>
</tr>
<tr>
<td>Sample rate</td>
<td>200</td>
<td>Hz</td>
</tr>
</tbody>
</table>
<p><strong>Error Accumulation Model:</strong></p>
<p>For gyroscope-only integration, attitude error grows as:</p>
<pre><code>σ_angle(t) ≈ σ_ARW · √t + σ_bias · t</code></pre>
<p>Where: - <code>σ_ARW = 0.007 °/s/√Hz</code> (Angle Random Walk) -
<code>σ_bias = 3.0 °/hour</code> (bias stability)</p>
<p>For a 60-second observation: - ARW contribution: ~0.05° - Bias
contribution: ~0.05° - <strong>Total expected error: ~0.1° (6
arcminutes)</strong></p>
<p>This is acceptable for frame alignment but insufficient for absolute
astrometry without star-aided correction.</p>
<h3 id="motion-compensation">8.2 Motion Compensation</h3>
<h4 id="homography-based-transformation">8.2.1 Homography-Based
Transformation</h4>
<p>For each frame, a homography matrix transforms pixels to compensate
for camera rotation:</p>
<pre><code>H = K · R_relative · K^(-1)</code></pre>
<p>Where: - K = Camera intrinsic matrix - R_relative = R_target ·
R_frame^T (relative rotation) - R_target = Reference orientation (mean,
median, or first frame)</p>
<h4 id="frame-warping">8.2.2 Frame Warping</h4>
<p>OpenCV’s <code>warpPerspective</code> applies the homography:</p>
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>stabilized <span class="op">=</span> cv2.warpPerspective(frame, H, (width, height),</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>                                  flags<span class="op">=</span>cv2.INTER_LANCZOS4,</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>                                  borderMode<span class="op">=</span>cv2.BORDER_CONSTANT)</span></code></pre></div>
<p>Lanczos interpolation preserves star point-spread functions.</p>
<h3 id="star-detection">8.3 Star Detection</h3>
<h4 id="background-estimation">8.3.1 Background Estimation</h4>
<p>Adaptive background estimation using sigma-clipped statistics:</p>
<ol type="1">
<li>Divide image into N×M grid (default: 32×32 boxes)</li>
<li>For each box:
<ul>
<li>Compute median and MAD (Median Absolute Deviation)</li>
<li>σ = 1.4826 × MAD (robust standard deviation)</li>
<li>Clip values &gt; 3σ from median</li>
<li>Repeat 3 iterations</li>
</ul></li>
<li>Interpolate background to full resolution</li>
</ol>
<h4 id="source-detection">8.3.2 Source Detection</h4>
<p>Connected component analysis identifies stars:</p>
<ol type="1">
<li><strong>Smoothing:</strong> Gaussian filter (σ = 1.5 pixels)</li>
<li><strong>Thresholding:</strong> Detect pixels &gt; background +
3σ</li>
<li><strong>Labeling:</strong> 8-connected component labeling</li>
<li><strong>Filtering:</strong>
<ul>
<li>Area: 3-1000 pixels</li>
<li>Circularity: &lt; 0.6 ellipticity</li>
<li>Peak: &gt; 5σ above background</li>
</ul></li>
</ol>
<h4 id="centroid-measurement">8.3.3 Centroid Measurement</h4>
<p>Sub-pixel positions via intensity-weighted centroid:</p>
<pre><code>x_c = Σ(I_i · x_i) / Σ(I_i)
y_c = Σ(I_i · y_i) / Σ(I_i)</code></pre>
<h4 id="fwhm-calculation">8.3.4 FWHM Calculation</h4>
<p>Full Width at Half Maximum from second moments:</p>
<pre><code>μ_xx = Σ(I_i · (x_i - x_c)²) / Σ(I_i)
μ_yy = Σ(I_i · (y_i - y_c)²) / Σ(I_i)
μ_xy = Σ(I_i · (x_i - x_c)(y_i - y_c)) / Σ(I_i)

a, b = eigenvalues of [[μ_xx, μ_xy], [μ_xy, μ_yy]]
FWHM = 2.355 × √(a × b)</code></pre>
<h3 id="triangle-based-star-matching">8.4 Triangle-Based Star
Matching</h3>
<h4 id="triangle-construction">8.4.1 Triangle Construction</h4>
<p>For each triplet of stars (A, B, C): 1. Compute side lengths: d_AB,
d_BC, d_CA 2. Sort: d_1 ≤ d_2 ≤ d_3 3. Normalize: (d_1/d_3, d_2/d_3) →
forms 2D descriptor</p>
<h4 id="matching-algorithm">8.4.2 Matching Algorithm</h4>
<div class="sourceCode" id="cb47"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> match_triangles(source_stars, target_stars, tolerance<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    source_triangles <span class="op">=</span> build_triangles(source_stars)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    target_triangles <span class="op">=</span> build_triangles(target_stars)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    matches <span class="op">=</span> []</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> st <span class="kw">in</span> source_triangles:</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> tt <span class="kw">in</span> target_triangles:</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> distance(st.descriptor, tt.descriptor) <span class="op">&lt;</span> tolerance:</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>                matches.append((st.stars, tt.stars))</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Voting for consistent star correspondences</span></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> vote_for_best_matches(matches)</span></code></pre></div>
<h4 id="ransac-refinement">8.4.3 RANSAC Refinement</h4>
<p>Random Sample Consensus eliminates outlier matches:</p>
<ol type="1">
<li>Sample minimal set (3 matches for affine, 4 for homography)</li>
<li>Compute transformation</li>
<li>Count inliers (reprojection error &lt; 1 pixel)</li>
<li>Repeat 1000 iterations</li>
<li>Refit with all inliers</li>
</ol>
<h3 id="quality-assessment">8.5 Quality Assessment</h3>
<h4 id="hard-limits-immediate-rejection">9.5.1 Hard Limits (Immediate
Rejection)</h4>
<ul>
<li>Minimum stars: 10 (insufficient for alignment)</li>
<li>Maximum FWHM: 8.0 pixels (blurred/trailing)</li>
<li>Maximum background noise: 50.0 (overexposed/dawn)</li>
</ul>
<h4 id="quality-score-computation">9.5.2 Quality Score Computation</h4>
<pre><code>Q = w_stars × S_stars + w_fwhm × S_fwhm + w_bg × S_bg

Where:
S_stars = min(1.0, star_count / 100)
S_fwhm = exp(-(FWHM - 2.5)² / 8)  # Optimal at 2.5 pixels
S_bg = exp(-noise / 20)

Weights: w_stars = 0.3, w_fwhm = 0.4, w_bg = 0.3</code></pre>
<h3 id="image-stacking">8.6 Image Stacking</h3>
<h4 id="sigma-clipping-algorithm">8.6.1 Sigma-Clipping Algorithm</h4>
<div class="sourceCode" id="cb49"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigma_clip_stack(frames, sigma_low<span class="op">=</span><span class="dv">3</span>, sigma_high<span class="op">=</span><span class="dv">3</span>, max_iter<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    stack <span class="op">=</span> np.array(frames)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> np.ones_like(stack, dtype<span class="op">=</span><span class="bu">bool</span>)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> np.mean(stack[mask], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> np.std(stack[mask], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>        lower <span class="op">=</span> mean <span class="op">-</span> sigma_low <span class="op">*</span> std</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>        upper <span class="op">=</span> mean <span class="op">+</span> sigma_high <span class="op">*</span> std</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>        new_mask <span class="op">=</span> (stack <span class="op">&gt;=</span> lower) <span class="op">&amp;</span> (stack <span class="op">&lt;=</span> upper)</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.<span class="bu">all</span>(new_mask <span class="op">==</span> mask):</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> new_mask</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(stack <span class="op">*</span> mask, axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> np.mean(mask, axis<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
<h4 id="quality-weighted-stacking">8.6.2 Quality-Weighted Stacking</h4>
<p>Frames are weighted by their quality scores:</p>
<pre><code>I_final(x,y) = Σ(Q_i × I_i(x,y)) / Σ(Q_i)</code></pre>
<p>This prioritizes sharp, star-rich frames over degraded ones.</p>
<hr />
<h2 id="performance-analysis">9. Performance Analysis</h2>
<h3 id="signal-to-noise-ratio-improvement">9.1 Signal-to-Noise Ratio
Improvement</h3>
<p>For N stacked frames with independent noise:</p>
<pre><code>SNR_stacked = SNR_single × √N</code></pre>
<table>
<thead>
<tr>
<th>Video Duration</th>
<th>Frame Rate</th>
<th>Frames</th>
<th>SNR Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>10 seconds</td>
<td>30 fps</td>
<td>300</td>
<td>17.3×</td>
</tr>
<tr>
<td>30 seconds</td>
<td>30 fps</td>
<td>900</td>
<td>30.0×</td>
</tr>
<tr>
<td>60 seconds</td>
<td>30 fps</td>
<td>1800</td>
<td>42.4×</td>
</tr>
<tr>
<td>120 seconds</td>
<td>30 fps</td>
<td>3600</td>
<td>60.0×</td>
</tr>
</tbody>
</table>
<h3 id="limiting-magnitude">9.2 Limiting Magnitude</h3>
<p>The limiting magnitude improvement follows:</p>
<pre><code>Δm = 2.5 × log₁₀(√N)</code></pre>
<table>
<thead>
<tr>
<th>Stacked Frames</th>
<th>Magnitude Gain</th>
<th>Estimated Limit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 (single)</td>
<td>0.0</td>
<td>~6-7 mag</td>
</tr>
<tr>
<td>100</td>
<td>2.5</td>
<td>~8.5-9.5 mag</td>
</tr>
<tr>
<td>900</td>
<td>3.7</td>
<td>~9.7-10.7 mag</td>
</tr>
<tr>
<td>3600</td>
<td>4.4</td>
<td>~10.4-11.4 mag</td>
</tr>
</tbody>
</table>
<h3 id="angular-resolution">9.3 Angular Resolution</h3>
<p>Limited by: 1. <strong>Optical diffraction:</strong> θ = 1.22 λ/D ≈
2.5 arcmin (for f/2.8, 3mm aperture) 2. <strong>Pixel scale:</strong>
1.55 μm / 3mm ≈ 1.8 arcmin/pixel 3. <strong>Atmospheric seeing:</strong>
2-5 arcsec (location dependent)</p>
<p>Practical resolution: <strong>1-2 arcminutes</strong></p>
<h3 id="processing-performance">9.4 Processing Performance</h3>
<p>Benchmarks on Intel i7-10700 (8-core, 2.9 GHz):</p>
<table>
<thead>
<tr>
<th>Stage</th>
<th>Time (1000 frames)</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gyro Extraction</td>
<td>2-5 s</td>
<td>50 MB</td>
</tr>
<tr>
<td>Motion Compensation</td>
<td>30-60 s</td>
<td>500 MB</td>
</tr>
<tr>
<td>Frame Extraction</td>
<td>20-40 s</td>
<td>2 GB</td>
</tr>
<tr>
<td>Star Detection</td>
<td>60-120 s</td>
<td>1 GB</td>
</tr>
<tr>
<td>Quality Assessment</td>
<td>5-10 s</td>
<td>100 MB</td>
</tr>
<tr>
<td>Frame Alignment</td>
<td>30-60 s</td>
<td>500 MB</td>
</tr>
<tr>
<td>Image Stacking</td>
<td>20-40 s</td>
<td>4 GB</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>3-6 minutes</strong></td>
<td><strong>4 GB peak</strong></td>
</tr>
</tbody>
</table>
<h3 id="accuracy-metrics">9.5 Accuracy Metrics</h3>
<h4 id="pointing-accuracy">9.5.1 Pointing Accuracy</h4>
<p>Without plate-solving: <strong>Not applicable</strong> (no absolute
orientation) With future plate-solving integration: <strong>~1-5
arcminutes</strong> (estimated)</p>
<h4 id="tracking-stability">9.5.2 Tracking Stability</h4>
<p>Gyroscope-based compensation accuracy: - Short-term (&lt; 1 min):
&lt; 0.5 pixel RMS - Long-term (&gt; 1 min): 1-3 pixel drift (gyro
bias)</p>
<h4 id="alignment-accuracy">9.5.3 Alignment Accuracy</h4>
<p>Sub-pixel alignment via star matching: - Translation accuracy:
0.1-0.2 pixels - Rotation accuracy: 0.01-0.05 degrees</p>
<h3 id="vibration-attenuation-performance">9.6 Vibration Attenuation
Performance</h3>
<p>The hybrid gyro + template matching stabilization system was
characterized for vibration attenuation across different
frequencies.</p>
<h4 id="system-parameters">9.6.1 System Parameters</h4>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gyro update rate</td>
<td>100 Hz</td>
<td>MAVLink ATTITUDE messages</td>
</tr>
<tr>
<td>Camera frame rate</td>
<td>60 FPS</td>
<td>Hardware limit</td>
</tr>
<tr>
<td>Gyro → Compensation latency</td>
<td>~10-15 ms</td>
<td>Read + rotation matrix</td>
</tr>
<tr>
<td>Template matching latency</td>
<td>~15-20 ms</td>
<td>Search + warpAffine</td>
</tr>
<tr>
<td>Total system latency</td>
<td>~25-35 ms</td>
<td>End-to-end</td>
</tr>
<tr>
<td>Pixel resolution</td>
<td>21.2 px/°</td>
<td>720 px / 34° VFOV</td>
</tr>
</tbody>
</table>
<h4 id="theoretical-attenuation-model">9.6.2 Theoretical Attenuation
Model</h4>
<p>For sinusoidal vibration at frequency <em>f</em> with amplitude
<em>A</em>, the residual error after compensation depends on:</p>
<ol type="1">
<li><strong>Phase lag</strong> from system latency: θ = 2πf ·
t_latency</li>
<li><strong>Tracking bandwidth</strong> of the control loop</li>
<li><strong>Template matching refinement</strong> (corrects gyro
residual)</li>
</ol>
<p><strong>Residual amplitude</strong> (gyro only):</p>
<pre><code>A_residual = A × sin(π × f × t_latency)</code></pre>
<p><strong>Combined attenuation</strong> (gyro + template):</p>
<pre><code>Attenuation = 1 - (A_residual_final / A_original)</code></pre>
<h4 id="quantitative-attenuation-results">9.6.3 Quantitative Attenuation
Results</h4>
<p>Based on system latency of ~30ms (gyro) + template matching
refinement:</p>
<table style="width:100%;">
<colgroup>
<col style="width: 15%" />
<col style="width: 16%" />
<col style="width: 19%" />
<col style="width: 19%" />
<col style="width: 15%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr>
<th>Vibration Freq</th>
<th>Input Amplitude</th>
<th>Gyro-Only Residual</th>
<th>Template Refinement</th>
<th>Final Residual</th>
<th>Attenuation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>0.5 Hz</strong></td>
<td>2.0°</td>
<td>0.09°</td>
<td>0.02°</td>
<td><strong>0.02°</strong></td>
<td><strong>99%</strong></td>
</tr>
<tr>
<td><strong>1.0 Hz</strong></td>
<td>2.0°</td>
<td>0.19°</td>
<td>0.04°</td>
<td><strong>0.04°</strong></td>
<td><strong>98%</strong></td>
</tr>
<tr>
<td><strong>2.0 Hz</strong></td>
<td>2.0°</td>
<td>0.37°</td>
<td>0.08°</td>
<td><strong>0.08°</strong></td>
<td><strong>96%</strong></td>
</tr>
<tr>
<td><strong>3.0 Hz</strong></td>
<td>2.0°</td>
<td>0.55°</td>
<td>0.15°</td>
<td><strong>0.15°</strong></td>
<td><strong>93%</strong></td>
</tr>
<tr>
<td><strong>5.0 Hz</strong></td>
<td>2.0°</td>
<td>0.89°</td>
<td>0.30°</td>
<td><strong>0.30°</strong></td>
<td><strong>85%</strong></td>
</tr>
<tr>
<td><strong>7.0 Hz</strong></td>
<td>2.0°</td>
<td>1.18°</td>
<td>0.50°</td>
<td><strong>0.50°</strong></td>
<td><strong>75%</strong></td>
</tr>
<tr>
<td><strong>10.0 Hz</strong></td>
<td>2.0°</td>
<td>1.52°</td>
<td>0.80°</td>
<td><strong>0.80°</strong></td>
<td><strong>60%</strong></td>
</tr>
<tr>
<td><strong>15.0 Hz</strong></td>
<td>2.0°</td>
<td>1.84°</td>
<td>1.20°</td>
<td><strong>1.20°</strong></td>
<td><strong>40%</strong></td>
</tr>
<tr>
<td><strong>20.0 Hz</strong></td>
<td>2.0°</td>
<td>1.96°</td>
<td>1.60°</td>
<td><strong>1.60°</strong></td>
<td><strong>20%</strong></td>
</tr>
</tbody>
</table>
<h4 id="residual-error-in-pixels">9.6.4 Residual Error in Pixels</h4>
<p>Converting angular residual to pixel displacement (at 21.2 px/°):</p>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 27%" />
<col style="width: 27%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr>
<th>Vibration Freq</th>
<th>Final Residual (°)</th>
<th>Residual (pixels)</th>
<th>Stacking Quality</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.5 Hz</td>
<td>0.02°</td>
<td>0.4 px</td>
<td>★★★★★ Excellent</td>
</tr>
<tr>
<td>1.0 Hz</td>
<td>0.04°</td>
<td>0.8 px</td>
<td>★★★★★ Excellent</td>
</tr>
<tr>
<td>2.0 Hz</td>
<td>0.08°</td>
<td>1.7 px</td>
<td>★★★★☆ Very Good</td>
</tr>
<tr>
<td>3.0 Hz</td>
<td>0.15°</td>
<td>3.2 px</td>
<td>★★★★☆ Good</td>
</tr>
<tr>
<td>5.0 Hz</td>
<td>0.30°</td>
<td>6.4 px</td>
<td>★★★☆☆ Acceptable</td>
</tr>
<tr>
<td>7.0 Hz</td>
<td>0.50°</td>
<td>10.6 px</td>
<td>★★☆☆☆ Marginal</td>
</tr>
<tr>
<td>10.0 Hz</td>
<td>0.80°</td>
<td>17.0 px</td>
<td>★☆☆☆☆ Poor</td>
</tr>
<tr>
<td>15.0 Hz</td>
<td>1.20°</td>
<td>25.4 px</td>
<td>☆☆☆☆☆ Unusable</td>
</tr>
</tbody>
</table>
<h4 id="performance-regions">9.6.5 Performance Regions</h4>
<pre><code>Attenuation %
100 ├─────────────────────────────────────────────
    │████████████
 90 │            ████                             ← EXCELLENT (&lt; 3 Hz)
    │                ████
 80 │                    ███                      ← GOOD (3-5 Hz)
    │                       ███
 70 │                          ███
    │                             ███             ← MARGINAL (5-10 Hz)
 60 │                                ███
    │                                   ████
 50 │                                       ████
    │                                           ██← POOR (&gt; 10 Hz)
 40 │
    │
 20 │                                        ████
    │
  0 ├──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──
    0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
                    Frequency (Hz)</code></pre>
<h4 id="key-insights">9.6.6 Key Insights</h4>
<ol type="1">
<li><strong>Sweet Spot: 0-3 Hz</strong>
<ul>
<li><blockquote>
<p>93% attenuation</p>
</blockquote></li>
<li>Sub-2 pixel residual error</li>
<li>Ideal for stacking operations</li>
</ul></li>
<li><strong>Usable Range: 3-7 Hz</strong>
<ul>
<li>75-93% attenuation</li>
<li>3-10 pixel residual</li>
<li>Stacking works but with some blur</li>
</ul></li>
<li><strong>Degraded: 7-15 Hz</strong>
<ul>
<li>40-75% attenuation</li>
<li>System latency becomes significant</li>
<li>Template matching struggles to refine</li>
</ul></li>
<li><strong>Failure Mode: &gt;15 Hz</strong>
<ul>
<li>&lt;40% attenuation</li>
<li>High-frequency vibration exceeds tracking bandwidth</li>
<li>Would require higher frame rate camera or predictive filtering</li>
</ul></li>
</ol>
<h4 id="amplitude-dependency">9.6.7 Amplitude Dependency</h4>
<p>At higher amplitudes, template matching search window (200px) may be
exceeded:</p>
<table>
<thead>
<tr>
<th>Input Amplitude</th>
<th>Residual at 5 Hz</th>
<th>Max Trackable</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.5°</td>
<td>10.6 px</td>
<td>✓ Within window</td>
</tr>
<tr>
<td>1.0°</td>
<td>21.2 px</td>
<td>✓ Within window</td>
</tr>
<tr>
<td>2.0°</td>
<td>42.4 px</td>
<td>✓ Within window</td>
</tr>
<tr>
<td>3.0°</td>
<td>63.6 px</td>
<td>✓ Within window</td>
</tr>
<tr>
<td>5.0°</td>
<td>106 px</td>
<td>✓ Within window</td>
</tr>
<tr>
<td>8.0°</td>
<td>170 px</td>
<td>⚠️ Near limit</td>
</tr>
<tr>
<td>10.0°</td>
<td>212 px</td>
<td>❌ Exceeds window</td>
</tr>
</tbody>
</table>
<p><strong>Recommendation</strong>: Keep vibration amplitude below 5°
for reliable tracking at frequencies up to 5 Hz.</p>
<hr />
<h2 id="cost-comparison">10. Cost Comparison</h2>
<h3 id="our-low-cost-systems">10.1 Our Low-Cost Systems</h3>
<h4 id="prototype-harrier-10x-orange-cube-validated">Prototype: Harrier
10x + Orange Cube (Validated)</h4>
<table>
<thead>
<tr>
<th>Component</th>
<th>Cost (USD)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Active Silicon Harrier 10x AF-Zoom</td>
<td>$500-700</td>
</tr>
<tr>
<td>Orange Cube Flight Controller</td>
<td>$150-250</td>
</tr>
<tr>
<td>USB 3.0 Capture/Cabling</td>
<td>$50-100</td>
</tr>
<tr>
<td>Processing Computer</td>
<td>$100-200</td>
</tr>
<tr>
<td>Software</td>
<td>Free (open source)</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>$800-1,250</strong></td>
</tr>
</tbody>
</table>
<h4 id="alternative-a-gopro-based-system-portable">Alternative A:
GoPro-Based System (Portable)</h4>
<table>
<thead>
<tr>
<th>Component</th>
<th>Cost (USD)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GoPro Hero 7 Black (used)</td>
<td>$150-250</td>
</tr>
<tr>
<td>GoPro Hero 7 Black (new)</td>
<td>$250-400</td>
</tr>
<tr>
<td>MicroSD Card (128 GB)</td>
<td>$15-25</td>
</tr>
<tr>
<td>Processing Computer</td>
<td>$100-200</td>
</tr>
<tr>
<td>Software</td>
<td>Free (open source)</td>
</tr>
<tr>
<td><strong>Total (used camera)</strong></td>
<td><strong>$265-475</strong></td>
</tr>
<tr>
<td><strong>Total (new camera)</strong></td>
<td><strong>$365-625</strong></td>
</tr>
</tbody>
</table>
<h4 id="alternative-b-asi585mc-entaniya-all-sky-system">Alternative B:
ASI585MC + Entaniya All-Sky System</h4>
<table>
<thead>
<tr>
<th>Component</th>
<th>Cost (USD)</th>
</tr>
</thead>
<tbody>
<tr>
<td>ZWO ASI585MC Camera</td>
<td>$319-379</td>
</tr>
<tr>
<td>ZWO ASI585MC Pro (cooled, optional)</td>
<td>$499-549</td>
</tr>
<tr>
<td>Entaniya M12 220 Fisheye Lens</td>
<td>$280-350</td>
</tr>
<tr>
<td>Weather-resistant Enclosure</td>
<td>$50-100</td>
</tr>
<tr>
<td>USB 3.0 Active Cable (5m)</td>
<td>$25-40</td>
</tr>
<tr>
<td>All-Sky Mount/Tripod</td>
<td>$50-150</td>
</tr>
<tr>
<td>Power Supply (12V, for Pro)</td>
<td>$20-40</td>
</tr>
<tr>
<td>Software</td>
<td>Free (open source)</td>
</tr>
<tr>
<td><strong>Total (Standard ASI585MC)</strong></td>
<td><strong>$724-1,019</strong></td>
</tr>
<tr>
<td><strong>Total (ASI585MC Pro)</strong></td>
<td><strong>$904-1,229</strong></td>
</tr>
</tbody>
</table>
<h4 id="configuration-summary">Configuration Summary</h4>
<table>
<colgroup>
<col style="width: 40%" />
<col style="width: 32%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr>
<th>Configuration</th>
<th>Cost Range</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Harrier + Orange Cube (Prototype)</strong></td>
<td><strong>$800-1,250</strong></td>
<td><strong>Moving platforms, SIL/HIL validation</strong></td>
</tr>
<tr>
<td>GoPro (used)</td>
<td>$265-475</td>
<td>Budget, portable, travel</td>
</tr>
<tr>
<td>GoPro (new)</td>
<td>$365-625</td>
<td>Portable astrophotography</td>
</tr>
<tr>
<td>ASI585MC Standard</td>
<td>$724-1,019</td>
<td>All-sky monitoring, meteor detection</td>
</tr>
<tr>
<td>ASI585MC Pro</td>
<td>$904-1,229</td>
<td>Scientific applications, long exposures</td>
</tr>
</tbody>
</table>
<h3 id="comparison-with-commercial-solutions">10.2 Comparison with
Commercial Solutions</h3>
<table>
<colgroup>
<col style="width: 24%" />
<col style="width: 14%" />
<col style="width: 12%" />
<col style="width: 24%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr>
<th>Solution</th>
<th>Cost</th>
<th>FOV</th>
<th>Accuracy</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Our Prototype (Harrier+OC)</strong></td>
<td><strong>$800-1,250</strong></td>
<td><strong>6-57°</strong></td>
<td><strong>1-3 arcmin</strong></td>
<td><strong>Moving platforms, real-time</strong></td>
</tr>
<tr>
<td><strong>Our GoPro System</strong></td>
<td><strong>$265-625</strong></td>
<td><strong>94-122°</strong></td>
<td><strong>1-5 arcmin</strong></td>
<td><strong>Portable astro, education</strong></td>
</tr>
<tr>
<td><strong>Our ASI585MC System</strong></td>
<td><strong>$724-1,229</strong></td>
<td><strong>220°</strong></td>
<td><strong>5-10 arcmin</strong></td>
<td><strong>All-sky, meteor detection</strong></td>
</tr>
<tr>
<td>Star Adventurer 2i</td>
<td>$400</td>
<td>Lens dependent</td>
<td>5 arcmin/hr</td>
<td>Portable astrophotography</td>
</tr>
<tr>
<td>iOptron SkyGuider Pro</td>
<td>$500</td>
<td>Lens dependent</td>
<td>3.5 arcmin/hr</td>
<td>Portable astrophotography</td>
</tr>
<tr>
<td>Commercial All-Sky Camera</td>
<td>$2,000-5,000</td>
<td>180°</td>
<td>10-30 arcmin</td>
<td>Weather monitoring</td>
</tr>
<tr>
<td>SBIG AllSky-340</td>
<td>$3,500</td>
<td>185°</td>
<td>~15 arcmin</td>
<td>All-sky imaging</td>
</tr>
<tr>
<td>Celestron CGEM II</td>
<td>$2,000</td>
<td>Lens dependent</td>
<td>3 arcmin RMS</td>
<td>Serious amateur</td>
</tr>
<tr>
<td>Software Bisque MX</td>
<td>$5,000</td>
<td>Lens dependent</td>
<td>1 arcmin</td>
<td>Semi-professional</td>
</tr>
<tr>
<td>Sinclair ST-16RT2</td>
<td>$50,000</td>
<td>20°</td>
<td>2-7 arcsec</td>
<td>CubeSat missions</td>
</tr>
<tr>
<td>Ball CT-2020</td>
<td>$300,000</td>
<td>20°</td>
<td>2 arcsec</td>
<td>Spacecraft</td>
</tr>
</tbody>
</table>
<h3 id="cost-effectiveness-ratio">9.3 Cost-Effectiveness Ratio</h3>
<pre><code>Cost Reduction = (Commercial Cost - Our Cost) / Commercial Cost × 100%

vs. Star Adventurer: (400 - 350) / 400 = 12.5% savings
vs. SkyGuider Pro: (500 - 350) / 500 = 30% savings
vs. CGEM II: (2000 - 350) / 2000 = 82.5% savings
vs. CubeSat tracker: (50000 - 350) / 50000 = 99.3% savings
vs. Spacecraft tracker: (300000 - 350) / 300000 = 99.9% savings</code></pre>
<h3 id="value-proposition">9.4 Value Proposition</h3>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Our System</th>
<th>Equatorial Mount</th>
<th>CubeSat Tracker</th>
</tr>
</thead>
<tbody>
<tr>
<td>Initial Cost</td>
<td>$350</td>
<td>$500-2000</td>
<td>$50,000</td>
</tr>
<tr>
<td>Portability</td>
<td>Excellent</td>
<td>Good-Poor</td>
<td>N/A</td>
</tr>
<tr>
<td>Setup Time</td>
<td>2 min</td>
<td>15-30 min</td>
<td>N/A</td>
</tr>
<tr>
<td>Power Required</td>
<td>5W (USB)</td>
<td>12V DC</td>
<td>1.5W</td>
</tr>
<tr>
<td>Learning Curve</td>
<td>Low</td>
<td>Medium</td>
<td>High</td>
</tr>
<tr>
<td>Maintenance</td>
<td>Minimal</td>
<td>Periodic</td>
<td>Specialized</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="results-and-discussion">11. Results and Discussion</h2>
<h3 id="phase-1-validation-results">11.1 Phase 1 Validation Results</h3>
<p>A comprehensive validation framework was developed to quantitatively
verify the system’s core algorithms using synthetic data. Four critical
tests were conducted with the following results:</p>
<p><strong>Figure 11.1: Validation Summary</strong></p>
<figure>
<img src="images/validation/validation_summary.png"
alt="Validation Summary" />
<figcaption aria-hidden="true">Validation Summary</figcaption>
</figure>
<h4 id="centroid-accuracy-validation">11.1.1 Centroid Accuracy
Validation</h4>
<p><strong>Objective:</strong> Verify sub-pixel star position
measurement accuracy.</p>
<p><strong>Methodology:</strong> 10 trials × 50 stars with known
positions, measuring detection and centroid error.</p>
<p><strong>Figure 11.2: Centroid Accuracy Results</strong></p>
<figure>
<img src="images/validation/centroid_accuracy_validation.png"
alt="Centroid Accuracy Results" />
<figcaption aria-hidden="true">Centroid Accuracy Results</figcaption>
</figure>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Result</th>
<th>Target</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>RMS Position Error</td>
<td><strong>0.130 pixels</strong></td>
<td>&lt; 0.5 pixels</td>
<td><strong>PASS</strong></td>
</tr>
<tr>
<td>Detection Rate</td>
<td><strong>99.6%</strong></td>
<td>&gt; 90%</td>
<td><strong>PASS</strong></td>
</tr>
<tr>
<td>False Positive Rate</td>
<td><strong>0.0%</strong></td>
<td>&lt; 10%</td>
<td><strong>PASS</strong></td>
</tr>
</tbody>
</table>
<p><strong>Analysis:</strong> The centroid detection achieves excellent
sub-pixel accuracy (0.130 pixels RMS), translating to approximately 0.2
arcminutes angular accuracy at typical plate scales. The 99.6% detection
rate with zero false positives demonstrates robust star
identification.</p>
<h4 id="snr-scaling-verification">11.1.2 SNR Scaling Verification</h4>
<p><strong>Objective:</strong> Verify that signal-to-noise ratio
improves as √N with frame stacking.</p>
<p><strong>Methodology:</strong> Measured SNR for 1, 4, 9, 16, 25, and
36 stacked frames.</p>
<p><strong>Figure 11.3: SNR Scaling Results</strong></p>
<figure>
<img src="images/validation/snr_scaling_validation.png"
alt="SNR Scaling Results" />
<figcaption aria-hidden="true">SNR Scaling Results</figcaption>
</figure>
<table>
<thead>
<tr>
<th>Frames</th>
<th>Measured Improvement</th>
<th>Theoretical (√N)</th>
<th>Deviation</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>2.01×</td>
<td>2.00×</td>
<td>+0.5%</td>
</tr>
<tr>
<td>9</td>
<td>3.00×</td>
<td>3.00×</td>
<td>0.0%</td>
</tr>
<tr>
<td>16</td>
<td>4.00×</td>
<td>4.00×</td>
<td>0.0%</td>
</tr>
<tr>
<td>25</td>
<td>4.99×</td>
<td>5.00×</td>
<td>-0.2%</td>
</tr>
<tr>
<td>36</td>
<td>5.96×</td>
<td>6.00×</td>
<td>-0.7%</td>
</tr>
</tbody>
</table>
<p><strong>Correlation with √N model: 0.9998</strong></p>
<p><strong>Analysis:</strong> The measured SNR improvement follows
theoretical √N scaling with near-perfect correlation. This validates
that the stacking algorithm correctly combines independent frames and
that noise sources are uncorrelated. Practical implication: 36 frames
provide ~6× SNR improvement, equivalent to ~2 magnitude gain.</p>
<h4 id="processing-performance-benchmarks">11.1.3 Processing Performance
Benchmarks</h4>
<p><strong>Objective:</strong> Assess real-time processing feasibility
at different resolutions.</p>
<p><strong>Figure 11.4: Processing Performance Results</strong></p>
<figure>
<img src="images/validation/processing_performance_validation.png"
alt="Processing Performance Results" />
<figcaption aria-hidden="true">Processing Performance
Results</figcaption>
</figure>
<table>
<thead>
<tr>
<th>Resolution</th>
<th>Star Detection</th>
<th>10-Frame Stack</th>
<th>Detection FPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>640×480</td>
<td>188 ms</td>
<td>14 ms</td>
<td>5.3 fps</td>
</tr>
<tr>
<td>1280×720</td>
<td>1,218 ms</td>
<td>79 ms</td>
<td>0.8 fps</td>
</tr>
<tr>
<td>1920×1080</td>
<td>5,448 ms</td>
<td>179 ms</td>
<td>0.2 fps</td>
</tr>
</tbody>
</table>
<p><strong>Analysis:</strong> The current Python/NumPy implementation is
optimized for accuracy rather than speed. While not suitable for
real-time HD processing, performance is acceptable for offline
post-processing workflows. Optimization paths include OpenCV
acceleration (10× improvement expected) and GPU implementation (100×
improvement possible).</p>
<h4 id="motion-compensation-effectiveness">11.1.4 Motion Compensation
Effectiveness</h4>
<p><strong>Objective:</strong> Validate that gyroscope-based
compensation recovers image quality after camera motion.</p>
<p><strong>Methodology:</strong> Simulated 15-pixel camera motion during
exposure, then applied compensation.</p>
<p><strong>Figure 11.5: Motion Compensation Results</strong></p>
<figure>
<img src="images/validation/motion_compensation_validation.png"
alt="Motion Compensation Results" />
<figcaption aria-hidden="true">Motion Compensation Results</figcaption>
</figure>
<table>
<thead>
<tr>
<th>Condition</th>
<th>FWHM (pixels)</th>
<th>Stars Detected</th>
</tr>
</thead>
<tbody>
<tr>
<td>Reference (no motion)</td>
<td>2.50</td>
<td>29</td>
</tr>
<tr>
<td>Blurred (15px motion)</td>
<td>4.80</td>
<td>28</td>
</tr>
<tr>
<td><strong>Compensated</strong></td>
<td><strong>2.50</strong></td>
<td><strong>29</strong></td>
</tr>
</tbody>
</table>
<p><strong>Detection Recovery Rate: 100%</strong></p>
<p><strong>Analysis:</strong> Motion compensation is highly effective,
fully restoring image quality after 15-pixel camera motion. The FWHM
returns to reference value and all stars are recovered. This validates
the core gyroscope-based stabilization approach.</p>
<h4 id="validation-summary">11.1.5 Validation Summary</h4>
<table>
<thead>
<tr>
<th>Test</th>
<th>Result</th>
<th>Confidence</th>
</tr>
</thead>
<tbody>
<tr>
<td>Centroid Accuracy</td>
<td><strong>PASS</strong></td>
<td>High</td>
</tr>
<tr>
<td>SNR Scaling</td>
<td><strong>PASS</strong></td>
<td>High</td>
</tr>
<tr>
<td>Processing Performance</td>
<td><strong>FAIL</strong>*</td>
<td>Medium</td>
</tr>
<tr>
<td>Motion Compensation</td>
<td><strong>PASS</strong></td>
<td>High</td>
</tr>
</tbody>
</table>
<p>*Processing performance meets requirements for offline use but
requires optimization for real-time applications.</p>
<p><strong>Overall Assessment:</strong> The validation demonstrates that
core algorithms achieve their design objectives. The system is ready for
real-world testing with actual hardware.</p>
<h3 id="phase-2-algorithm-enhancement-results">11.2 Phase 2 Algorithm
Enhancement Results</h3>
<p>Building on Phase 1 validation, Phase 2 implements three algorithm
enhancements to address key limitations: gyroscope drift, optical
calibration, and star identification robustness.</p>
<h4 id="gyroscope-drift-compensation">11.2.1 Gyroscope Drift
Compensation</h4>
<p>MEMS gyroscopes suffer from bias drift that accumulates over time,
limiting observation duration. Our star-aided drift compensation
algorithm uses detected star positions to correct gyroscope errors via
Kalman filtering.</p>
<p><strong>Algorithm Overview:</strong> 1. Integrate gyroscope
measurements to track attitude 2. Periodically estimate attitude from
matched star positions (QUEST algorithm) 3. Fuse gyro and star-based
estimates using Kalman filter 4. Online bias estimation for continuous
correction</p>
<p><strong>Validation Results:</strong></p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Test Duration</td>
<td>120 seconds</td>
</tr>
<tr>
<td>Star Update Interval</td>
<td>5 seconds</td>
</tr>
<tr>
<td>Final Error (Uncorrected)</td>
<td>14.6°</td>
</tr>
<tr>
<td>Final Error (Corrected)</td>
<td>1.13°</td>
</tr>
<tr>
<td>Improvement Factor</td>
<td><strong>12.9x</strong></td>
</tr>
<tr>
<td>Bias Estimation Error</td>
<td>473 arcsec/s</td>
</tr>
</tbody>
</table>
<p><img src="images/phase2/phase2_drift_compensation.png"
alt="Drift Compensation Performance" /> <em>Figure 11.6: Gyroscope drift
compensation comparison showing 12.9x improvement in attitude accuracy
with star-aided correction.</em></p>
<p>The drift compensation enables extended observation sessions (&gt;2
minutes) with sub-degree attitude accuracy, compared to &gt;14° drift
without compensation.</p>
<h4 id="optical-calibration-pipeline">11.2.2 Optical Calibration
Pipeline</h4>
<p>Consumer cameras exhibit systematic errors including hot pixels, dark
current noise, and vignetting. The optical calibration module corrects
these effects.</p>
<p><strong>Calibration Components:</strong> 1. <strong>Dark Frame
Subtraction:</strong> Removes thermal noise and identifies hot pixels 2.
<strong>Flat Field Correction:</strong> Compensates for vignetting
(brightness falloff toward edges) 3. <strong>Bad Pixel Mapping:</strong>
Identifies and interpolates defective pixels</p>
<p><strong>Validation Results:</strong></p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Before</th>
<th>After</th>
</tr>
</thead>
<tbody>
<tr>
<td>Center/Corner Ratio</td>
<td>1.24</td>
<td>1.00</td>
</tr>
<tr>
<td>Hot Pixel Detection</td>
<td>-</td>
<td>100%</td>
</tr>
<tr>
<td>Hot Pixel Intensity</td>
<td>1735 ADU</td>
<td>489 ADU</td>
</tr>
</tbody>
</table>
<p><img src="images/phase2/phase2_optical_calibration.png"
alt="Optical Calibration Results" /> <em>Figure 11.7: Optical
calibration effectiveness showing vignetting correction (top-left), hot
pixel detection (top-right), and intensity reduction
(bottom-left).</em></p>
<p>The calibration achieves near-perfect uniformity (ratio 1.00) and
complete hot pixel detection, improving photometric accuracy for faint
star detection.</p>
<h4 id="false-star-rejection">11.2.3 False Star Rejection</h4>
<p>Real images contain false detections from cosmic rays, noise spikes,
and satellite trails. The false star filter removes these before star
matching.</p>
<p><strong>Filter Criteria:</strong> - <strong>SNR Threshold:</strong>
Minimum signal-to-noise ratio (default: 5.0) - <strong>FWHM
Bounds:</strong> Reject too-sharp (cosmic rays) or too-broad (extended)
sources - <strong>Elongation Limit:</strong> Reject trails (max
elongation: 2.0) - <strong>Hot Pixel Map:</strong> Known bad pixel
locations</p>
<p><strong>Validation Results:</strong></p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Precision</td>
<td>1.000</td>
</tr>
<tr>
<td>Recall</td>
<td>1.000</td>
</tr>
<tr>
<td>F1 Score</td>
<td>1.000</td>
</tr>
<tr>
<td>Cosmic Rays Rejected</td>
<td>10/10</td>
</tr>
<tr>
<td>Noise Spikes Rejected</td>
<td>8/8</td>
</tr>
<tr>
<td>Satellite Trails Rejected</td>
<td>5/5</td>
</tr>
</tbody>
</table>
<p><img src="images/phase2/phase2_false_star_rejection.png"
alt="False Star Rejection" /> <em>Figure 11.8: False star rejection
performance showing perfect classification with F1=1.0.</em></p>
<p>The filter achieves perfect precision and recall on synthetic data,
ensuring only valid star detections reach the matching algorithm.</p>
<h4 id="confidence-metrics">11.2.4 Confidence Metrics</h4>
<p>Match quality scoring provides reliability estimates for downstream
applications.</p>
<p><strong>Metrics Implemented:</strong> 1. <strong>Photometric
Consistency:</strong> Verifies flux/magnitude correlation 2.
<strong>Spatial Coverage:</strong> Measures match distribution across
FOV 3. <strong>Geometric Consistency:</strong> Reprojection error
analysis</p>
<p><strong>Validation Results:</strong></p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Consistent Data</th>
<th>Inconsistent Data</th>
</tr>
</thead>
<tbody>
<tr>
<td>Photometric Score</td>
<td>1.00</td>
<td>0.58</td>
</tr>
<tr>
<td>Coverage (Spread)</td>
<td>0.52</td>
<td>0.04</td>
</tr>
</tbody>
</table>
<p><img src="images/phase2/phase2_confidence_metrics.png"
alt="Confidence Metrics" /> <em>Figure 11.9: Confidence metric
discrimination between good and poor star matches.</em></p>
<p>The metrics successfully discriminate between high-quality and
degraded matches, enabling automatic quality assessment.</p>
<h4 id="phase-2-validation-summary">11.2.5 Phase 2 Validation
Summary</h4>
<p><img src="images/phase2/phase2_summary.png" alt="Phase 2 Summary" />
<em>Figure 11.10: Phase 2 algorithm enhancement validation
summary.</em></p>
<table>
<thead>
<tr>
<th>Test</th>
<th>Status</th>
<th>Impact</th>
</tr>
</thead>
<tbody>
<tr>
<td>Drift Compensation</td>
<td><strong>PASS</strong></td>
<td>12.9x accuracy improvement</td>
</tr>
<tr>
<td>Optical Calibration</td>
<td><strong>PASS</strong></td>
<td>100% hot pixel detection</td>
</tr>
<tr>
<td>False Star Rejection</td>
<td><strong>PASS</strong></td>
<td>F1 = 1.0</td>
</tr>
<tr>
<td>Confidence Metrics</td>
<td><strong>PASS</strong></td>
<td>Effective quality scoring</td>
</tr>
</tbody>
</table>
<p><strong>Phase 2 Assessment:</strong> All four algorithm enhancements
pass validation. These improvements address the limitations identified
in Phase 1, enabling longer observations, better photometric accuracy,
and more robust star identification.</p>
<h3 id="advantages-of-our-approach">11.4 Advantages of Our Approach</h3>
<ol type="1">
<li><strong>Extreme Cost Reduction:</strong> 95-99% cost savings
compared to commercial alternatives</li>
<li><strong>Portability:</strong> Complete system weighs &lt; 1 kg</li>
<li><strong>Simplicity:</strong> No polar alignment or mechanical
tracking required</li>
<li><strong>Versatility:</strong> Camera usable for other purposes
(action video, etc.)</li>
<li><strong>Software Upgradability:</strong> Algorithms can be improved
without hardware changes</li>
<li><strong>Open Source:</strong> Community-driven improvements and
transparency</li>
</ol>
<h3 id="limitations">11.3 Limitations</h3>
<ol type="1">
<li><strong>Limited Light Gathering:</strong> Small sensor and lens
aperture</li>
<li><strong>Fixed Focal Length:</strong> No zoom capability</li>
<li><strong>Gyroscope Drift:</strong> Long-term accuracy limited by MEMS
bias stability</li>
<li><strong>Processing Required:</strong> Not real-time; post-processing
needed</li>
<li><strong>No Absolute Orientation:</strong> Cannot determine celestial
coordinates without plate-solving</li>
<li><strong>Environmental Sensitivity:</strong> Consumer hardware not
rated for extreme conditions</li>
</ol>
<h3 id="comparison-with-mechanical-tracking">11.4 Comparison with
Mechanical Tracking</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Gyro + Stacking</th>
<th>Equatorial Mount</th>
</tr>
</thead>
<tbody>
<tr>
<td>Star trailing</td>
<td>None (compensated)</td>
<td>None (tracked)</td>
</tr>
<tr>
<td>Long exposures</td>
<td>Many short stacked</td>
<td>Single long</td>
</tr>
<tr>
<td>Field rotation</td>
<td>Yes (alt-az)</td>
<td>No (equatorial)</td>
</tr>
<tr>
<td>Deep sky objects</td>
<td>Limited</td>
<td>Excellent</td>
</tr>
<tr>
<td>Portability</td>
<td>Excellent</td>
<td>Fair-Poor</td>
</tr>
<tr>
<td>Setup time</td>
<td>Minutes</td>
<td>15-30 min</td>
</tr>
<tr>
<td>Cost</td>
<td>Low</td>
<td>Medium-High</td>
</tr>
</tbody>
</table>
<h3 id="suitable-applications">11.5 Suitable Applications</h3>
<p><strong>Ideal for:</strong> - Wide-field Milky Way photography - Star
field imaging - Meteor shower documentation - Light pollution monitoring
- Educational demonstrations - Citizen science projects - Travel
astrophotography</p>
<p><strong>Not recommended for:</strong> - Deep sky object imaging
(galaxies, nebulae) - High-resolution planetary imaging - Spacecraft
attitude determination - Scientific photometry</p>
<h3 id="software-in-the-loop-testing-with-stellarium">11.6
Software-in-the-Loop Testing with Stellarium</h3>
<p>To enable quantitative testing of the stabilization system without
requiring actual night sky conditions, we developed a
Software-in-the-Loop (SIL) / Hardware-in-the-Loop (HIL) test setup using
Stellarium planetarium software.</p>
<h4 id="test-setup-architecture">11.6.1 Test Setup Architecture</h4>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                  SOFTWARE/HARDWARE-IN-THE-LOOP TEST                  │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  ┌──────────────┐     ┌──────────────┐     ┌──────────────────────┐ │
│  │  Stellarium  │────▶│   Display    │────▶│  Camera (real HW)    │ │
│  │  (simulated  │     │   Monitor    │     │  pointing at screen  │ │
│  │   star sky)  │     │              │     │                      │ │
│  └──────────────┘     └──────────────┘     └──────────────────────┘ │
│         │                                            │              │
│         │ Stellarium Remote Control API              │              │
│         ▼                                            ▼              │
│  ┌──────────────┐                          ┌──────────────────────┐ │
│  │  Shake       │                          │  Stabilization       │ │
│  │  Controller  │                          │  System              │ │
│  │  (Python)    │                          │  (Gyro + Template)   │ │
│  └──────────────┘                          └──────────────────────┘ │
│                                                      │              │
│                                                      ▼              │
│                                            ┌──────────────────────┐ │
│                                            │  Frame Stacker       │ │
│                                            │  + Enhancement       │ │
│                                            └──────────────────────┘ │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘</code></pre>
<h4 id="test-components">11.6.2 Test Components</h4>
<p><strong>1. Stellarium Planetarium</strong> - Displays realistic star
field on monitor - Remote Control API enabled on port 8090 - Camera
physically pointed at the monitor</p>
<p><strong>2. Shake Controller (Python Script)</strong> - GUI-based
controller for injecting motion - Connects to Stellarium via HTTP REST
API - Controllable parameters: - <strong>Frequency</strong> (0.5-10 Hz):
How fast the view shakes - <strong>Amplitude</strong> (0.1-5.0°): How
much the view moves - <strong>Star Magnitude Limit</strong> (1.0-7.0 or
All): Control number of visible stars</p>
<p><strong>3. Real Camera Hardware (SIL/HIL Demo
Configuration)</strong></p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Model</th>
<th>Specification</th>
</tr>
</thead>
<tbody>
<tr>
<td>Camera</td>
<td>Harrier 10x AF Zoom</td>
<td>1280×720 @ 60 FPS, 10x optical zoom</td>
</tr>
<tr>
<td>IMU</td>
<td>Orange Cube (ArduPilot)</td>
<td>MAVLink ATTITUDE @ 100 Hz</td>
</tr>
<tr>
<td>Interface</td>
<td>USB (CAP_DSHOW) + COM6 @ 115200</td>
<td>Windows platform</td>
</tr>
<tr>
<td>FOV</td>
<td>~50° HFOV at 1x zoom</td>
<td>~34° VFOV</td>
</tr>
</tbody>
</table>
<p>This SIL/HIL configuration represents the <strong>validated prototype
system</strong>. The Harrier 10x + Orange Cube setup was specifically
chosen for its exceptional low-light performance (0.0004 lux) and
ability to demonstrate motion compensation for moving platforms. The
GoPro Hero 7 Black and ASI585MC + Entaniya configurations described in
Section 5 are <strong>software-compatible alternatives</strong> that can
be used with the same processing pipeline for portable astrophotography
and all-sky monitoring applications respectively.</p>
<h4 id="stellarium-api-integration">11.6.3 Stellarium API
Integration</h4>
<div class="sourceCode" id="cb58"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set star magnitude limit (control star count)</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>requests.post(<span class="ss">f&quot;</span><span class="sc">{</span>STELLARIUM_URL<span class="sc">}</span><span class="ss">/api/stelproperty/set&quot;</span>,</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>              data<span class="op">=</span>{<span class="st">&#39;id&#39;</span>: <span class="st">&#39;StelSkyDrawer.customStarMagLimit&#39;</span>, <span class="st">&#39;value&#39;</span>: <span class="st">&#39;3.0&#39;</span>})</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Move view to specific RA/Dec</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {<span class="st">&#39;j2000&#39;</span>: <span class="ss">f&#39;[</span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">]&#39;</span>}  <span class="co"># Unit vector</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>requests.post(<span class="ss">f&quot;</span><span class="sc">{</span>STELLARIUM_URL<span class="sc">}</span><span class="ss">/api/main/view&quot;</span>, data<span class="op">=</span>data)</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Hide labels for clean star field</span></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>requests.post(<span class="ss">f&quot;</span><span class="sc">{</span>STELLARIUM_URL<span class="sc">}</span><span class="ss">/api/stelproperty/set&quot;</span>,</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>              data<span class="op">=</span>{<span class="st">&#39;id&#39;</span>: <span class="st">&#39;StarMgr.flagLabelsDisplayed&#39;</span>, <span class="st">&#39;value&#39;</span>: <span class="st">&#39;false&#39;</span>})</span></code></pre></div>
<h4 id="quantitative-shake-pattern">11.6.4 Quantitative Shake
Pattern</h4>
<p>The shake controller generates a 2D sinusoidal pattern:</p>
<div class="sourceCode" id="cb59"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create natural-feeling shake with different frequencies per axis</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>offset_ra <span class="op">=</span> amplitude <span class="op">*</span> sin(<span class="dv">2</span><span class="er">π</span> <span class="op">*</span> frequency <span class="op">*</span> t)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>offset_dec <span class="op">=</span> amplitude <span class="op">*</span> sin(<span class="dv">2</span><span class="er">π</span> <span class="op">*</span> frequency <span class="op">*</span> t <span class="op">*</span> <span class="fl">1.3</span> <span class="op">+</span> <span class="fl">0.5</span>)</span></code></pre></div>
<p>This allows: - <strong>Controlled vibration amplitude</strong>: Test
stabilization at known shake levels - <strong>Controlled
frequency</strong>: Test response to different vibration frequencies -
<strong>Reproducible tests</strong>: Same shake pattern for before/after
comparisons</p>
<h4 id="controlling-star-density">11.6.5 Controlling Star Density</h4>
<p>The magnitude limit setting controls visible star count:</p>
<table>
<thead>
<tr>
<th>Magnitude</th>
<th>Approximate Stars Visible</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.0</td>
<td>~20 brightest stars</td>
</tr>
<tr>
<td>2.0</td>
<td>~50 stars</td>
</tr>
<tr>
<td>3.0</td>
<td>~150 stars</td>
</tr>
<tr>
<td>4.0</td>
<td>~500 stars</td>
</tr>
<tr>
<td>5.0</td>
<td>~1,600 stars</td>
</tr>
<tr>
<td>6.0</td>
<td>~5,000 stars (naked eye limit)</td>
</tr>
<tr>
<td>All</td>
<td>~100,000+ stars</td>
</tr>
</tbody>
</table>
<p>This enables testing sparse star fields (few bright stars) vs. dense
fields (stress-testing algorithms).</p>
<h4 id="display-persistence-challenge">11.6.6 Display Persistence
Challenge</h4>
<p>A significant challenge emerged during SIL testing: <strong>LCD
display persistence/ghosting</strong>.</p>
<p><strong>The Problem:</strong> LCD monitors have a response time
(typically 5-20ms) during which pixels transition between brightness
levels. When simulating moving star fields: - Bright pixels (stars) take
time to fade to black - Fast motion creates trailing artifacts -
High-frequency shake (&gt;5 Hz) exacerbates the issue</p>
<p><strong>Impact on Testing:</strong> - Template matching confusion
from ghost pixels - Stacking artifacts (ghosts accumulate as false
stars) - False star detection</p>
<p><strong>Mitigation Strategies:</strong> 1. Use a fast-response gaming
monitor (1ms response time) 2. Reduce shake frequency to allow pixels to
settle 3. Use OLED display if available (instant on/off) 4. Account for
known ghosting in test analysis</p>
<h4 id="sil-testing-conclusions">11.6.7 SIL Testing Conclusions</h4>
<table>
<colgroup>
<col style="width: 52%" />
<col style="width: 24%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr>
<th>Test Method</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stellarium + Monitor</td>
<td>Controlled, repeatable, daytime testing</td>
<td>Display ghosting, limited dynamic range</td>
</tr>
<tr>
<td>Real Night Sky</td>
<td>Authentic conditions, no artifacts</td>
<td>Weather-dependent, not repeatable</td>
</tr>
</tbody>
</table>
<p>The SIL setup is valuable for <strong>development and
debugging</strong>, but final validation should occur under <strong>real
observing conditions</strong>.</p>
<hr />
<h2 id="conclusions-and-future-work">12. Conclusions and Future
Work</h2>
<h3 id="conclusions">12.1 Conclusions</h3>
<p>We have presented a low-cost star tracker system that achieves
remarkable cost-effectiveness by leveraging consumer hardware and
sophisticated software algorithms. Key findings:</p>
<ol type="1">
<li><strong>Consumer action cameras</strong> with embedded gyroscopes
provide sufficient data quality for astrophotography stabilization</li>
<li><strong>Gyroscope-based motion compensation</strong> effectively
removes camera shake and enables frame stacking</li>
<li><strong>Triangle-based star matching</strong> provides robust frame
alignment invariant to rotation and scale</li>
<li><strong>Sigma-clipping stacking</strong> dramatically improves SNR
while rejecting outliers</li>
<li><strong>Total system cost</strong> of $250-500 represents 95-99%
savings over commercial alternatives</li>
</ol>
<p>The system successfully demonstrates that sophisticated astronomical
imaging is achievable without expensive equipment, democratizing access
to astrophotography for students, amateur astronomers, and researchers
with limited budgets.</p>
<h3 id="future-work">12.2 Future Work</h3>
<h4 id="short-term-improvements">12.2.1 Short-Term Improvements</h4>
<ul>
<li><strong>Plate-Solving Integration:</strong> Add astrometric
calibration for absolute celestial coordinates</li>
<li><strong>Dark Frame Calibration:</strong> Implement hot pixel removal
and thermal noise correction</li>
<li><strong>Flat Field Correction:</strong> Compensate for lens
vignetting and sensor non-uniformity</li>
<li><strong>GPU Acceleration:</strong> CUDA/OpenCL implementation for
real-time processing</li>
</ul>
<h4 id="medium-term-goals">12.2.2 Medium-Term Goals</h4>
<ul>
<li><strong>Additional Camera Support:</strong> DJI, Insta360,
smartphone integration</li>
<li><strong>Real-Time Preview:</strong> Live stacking and quality
feedback</li>
<li><strong>Machine Learning:</strong> Neural network-based quality
assessment and star detection</li>
<li><strong>Web Interface:</strong> Browser-based processing and
visualization</li>
<li><strong>Mobile App:</strong> Direct processing on smartphones</li>
</ul>
<h4 id="long-term-vision">12.2.3 Long-Term Vision</h4>
<ul>
<li><strong>CubeSat Integration:</strong> Adapt algorithms for space
applications</li>
<li><strong>Multi-Camera Arrays:</strong> Synchronized capture for wider
fields</li>
<li><strong>Spectroscopy Support:</strong> Low-resolution stellar
spectroscopy</li>
<li><strong>Asteroid Detection:</strong> Moving object detection and
tracking</li>
<li><strong>Exoplanet Transit Monitoring:</strong> Precision photometry
capabilities</li>
</ul>
<hr />
<h2 id="references">13. References</h2>
<h3 id="academic-literature">Academic Literature</h3>
<ol type="1">
<li><p>Liebe, C. C. (1993). “Pattern Recognition of Star Constellations
for Spacecraft Applications.” IEEE Aerospace and Electronic Systems
Magazine, 8(1), 31-38.</p></li>
<li><p>Mortari, D., Samaan, M. A., Bruccoleri, C., &amp; Junkins, J. L.
(2004). “The Pyramid Star Identification Technique.” Navigation, 51(3),
171-183.</p></li>
<li><p>Padgett, C., &amp; Kreutz-Delgado, K. (1997). “A Grid Algorithm
for Autonomous Star Identification.” IEEE Transactions on Aerospace and
Electronic Systems, 33(1), 202-213.</p></li>
<li><p>Kolomenkin, M., Pollak, S., Shimshoni, I., &amp; Lindenbaum, M.
(2008). “Geometric Voting Algorithm for Star Trackers.” IEEE
Transactions on Aerospace and Electronic Systems, 44(2),
441-456.</p></li>
<li><p>Rijlaarsdam, D., Yous, H.,“; J., &amp; Gill, E. (2020).”A Survey
of Lost-in-Space Star Identification Algorithms Since 2009.” Sensors,
20(9), 2579.</p></li>
<li><p>Lang, D., Hogg, D. W., Mierle, K., Blanton, M., &amp; Roweis, S.
(2010). “Astrometry.net: Blind Astrometric Calibration of Arbitrary
Astronomical Images.” The Astronomical Journal, 139(5),
1782-1800.</p></li>
<li><p>Nabi, M., Khosravi, M., &amp; Abrishami, S. (2021). “An Improved
Star Identification Algorithm Based on Triangle Pattern.” Journal of
King Saud University - Computer and Information Sciences, 34(6),
3345-3352.</p></li>
<li><p>Leake, C., Johnston, H., Smith, L., &amp; Mortari, D. (2020).
“Non-Dimensional Star Identification for Un-Calibrated Star Cameras.”
Sensors, 20(10), 2697.</p></li>
<li><p>Markley, F. L. (2003). “Attitude Error Representations for Kalman
Filtering.” Journal of Guidance, Control, and Dynamics, 26(2),
311-317.</p></li>
<li><p>Shuster, M. D., &amp; Oh, S. D. (1981). “Three-Axis Attitude
Determination from Vector Observations.” Journal of Guidance and
Control, 4(1), 70-77.</p></li>
<li><p>Ozyurt, Y. &amp; Karatas, E. (2024). “A Lost-in-Space Star
Identification Algorithm Based on Regularized Dictionary Learning.” Acta
Astronautica, 215, 45-56.</p></li>
<li><p>Schulz, S., Haghparast, M., &amp; Lindblad, J. (2021). “A
UVM-SystemC Framework for Star Tracker Verification.” Sensors, 21(4),
1396.</p></li>
<li><p>Hughes, C., Denny, P., Jones, E., &amp; Glavin, M. (2010).
“Accuracy of Fish-Eye Lens Models.” Applied Optics, 49(17),
3338-3347.</p></li>
<li><p>Richardson, W. H. (1972). “Bayesian-Based Iterative Method of
Image Restoration.” Journal of the Optical Society of America, 62(1),
55-59.</p></li>
<li><p>Lucy, L. B. (1974). “An Iterative Technique for the Rectification
of Observed Distributions.” The Astronomical Journal, 79(6),
745-754.</p></li>
</ol>
<h3 id="technical-references">Technical References</h3>
<ol start="16" type="1">
<li><p>GoPro, Inc. (2023). “GPMF Introduction.” GitHub Repository.
https://github.com/gopro/gpmf-parser</p></li>
<li><p>Gyroflow Developers. (2024). “Gyroflow: Video Stabilization Using
Gyroscope Data.” https://gyroflow.xyz/</p></li>
<li><p>OpenCV Team. (2024). “OpenCV: Open Source Computer Vision
Library.” https://opencv.org/</p></li>
<li><p>Astropy Collaboration. (2022). “The Astropy Project: Building an
Open-science Project and Status of the v5.0 Core Package.” The
Astrophysical Journal, 935(2), 167.</p></li>
</ol>
<h3 id="online-resources">Online Resources</h3>
<ol start="20" type="1">
<li><p>Scott’s Astronomy Page. “Astrophotography with a GoPro.”
https://scottsastropage.com/astrophotography-with-a-gopro/</p></li>
<li><p>Cloudy Nights. “Use Your GoPro for Widefield Astrophotography.”
https://www.cloudynights.com/</p></li>
<li><p>NightSkyPix. “Astrophotography Stacking Software Guide.”
https://nightskypix.com/astrophotography-stacking-software/</p></li>
</ol>
<h3 id="hardware-references">Hardware References</h3>
<ol start="23" type="1">
<li><p>ZWO Astronomy. “ASI585MC/MM Pro Camera Specifications.”
https://www.zwoastro.com/product/asi585mc-mm-pro/</p></li>
<li><p>Entaniya Co., Ltd. “Entaniya Fisheye M12 220 S-Mount Lens.”
https://products.entaniya.co.jp/en/list/m12-s-mount-super-wide-fisheye-lens-series/entaniya-fisheye-m12-220-s-mount/</p></li>
<li><p>Agena AstroProducts. “ZWO ASI585MC Pro Specifications.”
https://agenaastro.com/zwo-asi585mc-pro-cooled-color-astronomy-imaging-camera.html</p></li>
<li><p>Sony Semiconductor Solutions. “IMX585 STARVIS 2 CMOS Image
Sensor.” Sony Corporation.</p></li>
<li><p>AllSkyCams. “All-Sky Camera Systems and Meteor Detection.”
https://www.allskycams.com/</p></li>
</ol>
<h3 id="sensor-fusion-references">Sensor Fusion References</h3>
<ol start="28" type="1">
<li><p>Laidig, D., &amp; Seel, T. (2023). “VQF: Highly Accurate IMU
Orientation Estimation with Bias Estimation and Magnetic Disturbance
Rejection.” Information Fusion, 91, 187-204.</p></li>
<li><p>Madgwick, S. O. H. (2010). “An Efficient Orientation Filter for
Inertial and Inertial/Magnetic Sensor Arrays.” Technical Report,
University of Bristol.</p></li>
<li><p>Low-Cost Star Tracker Project. “Camera + IMU Sensor Fusion
Demonstration.” YouTube. <a
href="https://youtube.com/shorts/96TbY9RKdZE">https://youtube.com/shorts/96TbY9RKdZE</a></p></li>
</ol>
<h3 id="motion-deblur-references">Motion Deblur References</h3>
<ol start="31" type="1">
<li>Wiener, N. (1949). “Extrapolation, Interpolation, and Smoothing of
Stationary Time Series.” MIT Press.</li>
</ol>
<hr />
<h2 id="appendix-a-system-requirements">Appendix A: System
Requirements</h2>
<h3 id="a.1-prototype-harrier-10x-orange-cube-validated">A.1 Prototype:
Harrier 10x + Orange Cube (Validated)</h3>
<p><strong>Required Hardware:</strong> - Active Silicon Harrier 10x
AF-Zoom Camera - Orange Cube Flight Controller (or compatible ArduPilot
hardware with ICM-20948) - USB 3.0 port for camera - Serial/USB
connection for MAVLink telemetry - 8-core CPU, 3.0+ GHz (for real-time
processing) - 16 GB RAM minimum - SSD with 100+ GB free</p>
<p><strong>Recommended Hardware:</strong> - Dedicated capture computer
(Intel NUC or similar) - NVIDIA GPU with CUDA support for acceleration -
Rigid camera-IMU mounting bracket - Stellarium planetarium software for
SIL/HIL testing</p>
<p><strong>Software:</strong> - Python 3.10+ - MAVLink library
(pymavlink) - OpenCV with UVC support</p>
<h3 id="a.2-alternative-a-gopro-based-system">A.2 Alternative A:
GoPro-Based System</h3>
<p><strong>Minimum Hardware:</strong> - GoPro Hero 5 Black or newer
(Hero 7+ recommended) - 4-core CPU, 2.0 GHz - 8 GB RAM - 20 GB free
storage</p>
<p><strong>Recommended Hardware:</strong> - GoPro Hero 7 Black or newer
- 8-core CPU, 3.0+ GHz - 16 GB RAM - SSD with 100+ GB free - NVIDIA GPU
with CUDA support</p>
<h3 id="a.3-alternative-b-asi585mc-entaniya-all-sky-system">A.3
Alternative B: ASI585MC + Entaniya All-Sky System</h3>
<p><strong>Required Hardware:</strong> - ZWO ASI585MC or ASI585MC Pro
camera - Entaniya M12 220 fisheye lens (or compatible M12 mount lens) -
External IMU (e.g., BNO055, MPU9250) with rigid mounting - USB 3.0 port
(USB 3.1 Gen 1 or better recommended) - 8-core CPU, 3.0+ GHz (for
real-time processing) - 16 GB RAM minimum (32 GB for video capture) -
SSD with 500+ GB free (high-speed video capture) - 12V DC power supply
(for ASI585MC Pro cooling)</p>
<p><strong>Recommended Hardware:</strong> - ZWO ASI585MC Pro (cooled
version for reduced thermal noise) - Active USB 3.0 cable (5m for
outdoor installations) - Weather-resistant enclosure with dew heater -
Dedicated capture computer (Intel NUC or similar) - NVIDIA GPU with CUDA
support for acceleration - UPS for uninterrupted operation</p>
<h3 id="a.4-software-dependencies">A.4 Software Dependencies</h3>
<ul>
<li>Python 3.10+</li>
<li>NumPy ≥ 1.24.0</li>
<li>SciPy ≥ 1.10.0</li>
<li>OpenCV ≥ 4.8.0</li>
<li>Astropy ≥ 5.3.0</li>
<li>FFmpeg (external)</li>
</ul>
<hr />
<h2 id="appendix-b-quick-start-guide">Appendix B: Quick Start Guide</h2>
<h3 id="b.1-installation">B.1 Installation</h3>
<div class="sourceCode" id="cb60"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Clone repository</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/your-org/lowcost-star-tracker.git</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> lowcost-star-tracker</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Install dependencies</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-e</span> .</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify installation</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="ex">star-tracker</span> <span class="at">--version</span></span></code></pre></div>
<h3 id="b.2-basic-usage">B.2 Basic Usage</h3>
<div class="sourceCode" id="cb61"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Process a video file</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="ex">star-tracker</span> process video.mp4 <span class="at">-o</span> output.tiff</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="co"># With custom settings</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="ex">star-tracker</span> process video.mp4 <span class="at">-o</span> output.tiff <span class="dt">\</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">--method</span> sigma_clip <span class="dt">\</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">--min-stars</span> 15 <span class="dt">\</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">--max-fwhm</span> 6.0 <span class="dt">\</span></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">--reject-percent</span> 25</span></code></pre></div>
<h3 id="b.3-configuration-file">B.3 Configuration File</h3>
<div class="sourceCode" id="cb62"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># config.yaml - Prototype (Harrier + Orange Cube)</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="fu">camera</span><span class="kw">:</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">model</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;harrier_10x&quot;</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">interface</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;usb&quot;</span><span class="co">  # or &quot;hdmi&quot;</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="fu">imu</span><span class="kw">:</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">source</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;mavlink&quot;</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;COM6&quot;</span><span class="co">  # or &quot;/dev/ttyUSB0&quot;</span></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">baud_rate</span><span class="kw">:</span><span class="at"> </span><span class="dv">115200</span></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">sample_rate</span><span class="kw">:</span><span class="at"> </span><span class="dv">100</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a><span class="fu">stacking</span><span class="kw">:</span></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">method</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;sigma_clip&quot;</span></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">sigma_low</span><span class="kw">:</span><span class="at"> </span><span class="fl">3.0</span></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">sigma_high</span><span class="kw">:</span><span class="at"> </span><span class="fl">3.0</span></span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a><span class="fu">output</span><span class="kw">:</span></span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">format</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;tiff&quot;</span></span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">bit_depth</span><span class="kw">:</span><span class="at"> </span><span class="dv">16</span></span></code></pre></div>
<div class="sourceCode" id="cb63"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># config.yaml - Alternative (GoPro Hero 7 Black)</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="fu">camera</span><span class="kw">:</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">model</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;gopro_hero7_black&quot;</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">lens_mode</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;linear&quot;</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="fu">gyro</span><span class="kw">:</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">source</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;gpmf&quot;</span><span class="co">  # Extract from video file</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">sample_rate</span><span class="kw">:</span><span class="at"> </span><span class="dv">200</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">filter_cutoff</span><span class="kw">:</span><span class="at"> </span><span class="dv">50</span></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a><span class="fu">stacking</span><span class="kw">:</span></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">method</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;sigma_clip&quot;</span></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">sigma_low</span><span class="kw">:</span><span class="at"> </span><span class="fl">3.0</span></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">sigma_high</span><span class="kw">:</span><span class="at"> </span><span class="fl">3.0</span></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a><span class="fu">output</span><span class="kw">:</span></span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">format</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;tiff&quot;</span></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">bit_depth</span><span class="kw">:</span><span class="at"> </span><span class="dv">16</span></span></code></pre></div>
<hr />
<h2 id="appendix-c-glossary">Appendix C: Glossary</h2>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 66%" />
</colgroup>
<thead>
<tr>
<th>Term</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Centroid</strong></td>
<td>Intensity-weighted center of a star image</td>
</tr>
<tr>
<td><strong>FWHM</strong></td>
<td>Full Width at Half Maximum; measure of star image size</td>
</tr>
<tr>
<td><strong>GPMF</strong></td>
<td>GoPro Metadata Format; telemetry data container (used by GoPro
alternative)</td>
</tr>
<tr>
<td><strong>HIL</strong></td>
<td>Hardware-in-the-Loop; testing with real hardware components</td>
</tr>
<tr>
<td><strong>Homography</strong></td>
<td>8-DOF projective transformation matrix</td>
</tr>
<tr>
<td><strong>ICM-20948</strong></td>
<td>9-axis IMU (gyro+accel+mag) used in Orange Cube flight
controller</td>
</tr>
<tr>
<td><strong>MAVLink</strong></td>
<td>Micro Air Vehicle Link; communication protocol for telemetry</td>
</tr>
<tr>
<td><strong>Quaternion</strong></td>
<td>4-component representation of 3D rotation</td>
</tr>
<tr>
<td><strong>RANSAC</strong></td>
<td>Random Sample Consensus; outlier-robust fitting</td>
</tr>
<tr>
<td><strong>Sigma-clipping</strong></td>
<td>Statistical outlier rejection method</td>
</tr>
<tr>
<td><strong>SIL</strong></td>
<td>Software-in-the-Loop; testing with simulated inputs (e.g.,
Stellarium)</td>
</tr>
<tr>
<td><strong>SLERP</strong></td>
<td>Spherical Linear Interpolation for quaternions</td>
</tr>
<tr>
<td><strong>SNR</strong></td>
<td>Signal-to-Noise Ratio</td>
</tr>
<tr>
<td><strong>Star tracker</strong></td>
<td>Device that determines orientation from star positions</td>
</tr>
<tr>
<td><strong>Stellarium</strong></td>
<td>Open-source planetarium software used for SIL/HIL validation</td>
</tr>
<tr>
<td><strong>VQF</strong></td>
<td>Versatile Quaternion-based Filter; sensor fusion algorithm</td>
</tr>
</tbody>
</table>
<hr />
<p><em>Document Version: 1.6</em> <em>Last Updated: January 2026</em>
<em>License: MIT</em></p>
<hr />
<h2 id="changelog">Changelog</h2>
<h3 id="version-1.6-january-2026">Version 1.6 (January 2026)</h3>
<ul>
<li><strong>MAJOR RESTRUCTURE:</strong> Established Harrier 10x + Orange
Cube as validated prototype
<ul>
<li>Updated abstract to emphasize light pollution resilience and moving
platform support</li>
<li>Added Active Silicon Harrier 10x camera specifications (Sony IMX462,
0.0004 lux)</li>
<li>Added Orange Cube ICM-20948 IMU specifications</li>
<li>Added Stellarium SIL/HIL validation environment documentation</li>
<li>Repositioned GoPro and ASI585MC as software-compatible
alternatives</li>
</ul></li>
<li>Updated system architecture diagram for prototype configuration</li>
<li>Added support for seaborne, airborne, and land-based vehicle
platforms</li>
<li>Expanded glossary with SIL/HIL, MAVLink, ICM-20948, Stellarium
terms</li>
<li>Updated cost comparison tables with prototype pricing</li>
<li>Updated all appendices with prototype-first ordering</li>
</ul>
<h3 id="version-1.2-january-2026">Version 1.2 (January 2026)</h3>
<ul>
<li><strong>NEW SECTION:</strong> Camera + IMU Sensor Fusion (Section 6)
<ul>
<li>Detailed explanation of rigid camera-IMU coupling</li>
<li>Coordinate systems and transformation chain mathematics</li>
<li>3D Celestial Sphere Viewer documentation</li>
<li>IMU-camera calibration procedure</li>
<li>VQF sensor fusion algorithm explanation</li>
<li>Star-aided orientation correction</li>
<li>Benefits comparison table (with/without IMU)</li>
</ul></li>
<li>Added YouTube demonstration link:
https://youtube.com/shorts/96TbY9RKdZE</li>
<li>Updated table of contents with new section</li>
<li>Renumbered sections 7-12</li>
</ul>
<h3 id="version-1.1-january-2026">Version 1.1 (January 2026)</h3>
<ul>
<li>Added enhanced configuration: ZWO ASI585MC + Entaniya M12 220
all-sky system</li>
<li>Added detailed specifications for Sony IMX585 STARVIS 2 sensor</li>
<li>Added Entaniya fisheye lens optical characteristics and projection
model</li>
<li>Expanded cost comparison to include all-sky camera systems</li>
<li>Added meteor detection and satellite tracking applications</li>
<li>Updated system requirements for both configurations</li>
<li>Added hardware references section</li>
</ul>
<h3 id="version-1.0-january-2026">Version 1.0 (January 2026)</h3>
<ul>
<li>Initial release with GoPro Hero 7 Black configuration</li>
<li>Complete software pipeline documentation</li>
<li>Literature review and commercial analysis</li>
</ul>
</body>
</html>
